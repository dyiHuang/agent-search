global_batch_size: 512

data:
  tokenizer: null
  train_files: ./data/train.parquet
  val_files: ./data/test.parquet
  prompt_key: prompt
  max_prompt_length: 256
  max_response_length: 4096
  max_start_length: 512
  train_batch_size: ${global_batch_size}
  val_batch_size: 256
  return_raw_input_ids: False  # This should be set to true when the tokenizer between policy and rm differs
  return_raw_chat: False
  max_obs_length: 512
  train_data_num: null
  val_data_num: null
  shuffle_train_dataloader: True

deepspeed:
  train_batch_size: 512  # 全局 batch 大小（= 单进程 batch * DP_SIZE）
  train_micro_batch_size_per_gpu: ${actor.ppo_micro_batch_size}  # 单 GPU 微批大小（需满足：train_batch_size = 微批 * DP_SIZE * 梯度累积步数）,Np=4
  gradient_accumulation_steps: 32  # 梯度累积步数（RL 场景常用 1，避免延迟）
  gradient_clipping: 1.0  # 梯度裁剪阈值

  optimizer:
    type: "AdamW"
    params:
      lr: 2e-7
      betas: [0.9, 0.999]
      eps: 1e-8
      weight_decay: 1e-8

  zero_optimization:  # ZeRO 优化配置（与 TP 兼容，推荐 ZeRO-2/3 用于大模型）
    stage: 2  # 0=禁用，1=优化器状态分片，2=优化器+梯度分片，3=优化器+梯度+参数分片
    offload_optimizer:
      device: none  # 禁用 CPU 卸载（TP 场景下 GPU 卸载更高效）
    offload_param:
      device: none
    overlap_comm: True  # 通信与计算重叠（加速训练）
    contiguous_gradients: True  # 连续梯度（减少内存碎片）
    ignore_unused_parameters: True
    allgather_partitions: True  # 关键：聚合梯度到分片

  fp16:  # 混合精度配置（与 megatron-core 统一）
    enabled: False  # 禁用 FP16
  bf16:  # 混合精度配置（与 megatron-core 统一）
    enabled: True # 启用 BF16（推荐，与 megatron-core 兼容）

#  # 优化器配置（DeepSpeed 管理优化器，替代原生 PyTorch 优化器）
#  optimizer:
#    type: AdamW
#    params:  # 禁用 FP16
#      lr: 1e-5  #
#      weight_decay: 0.01  #
#      betas: [0.9, 0.95]  #
  # 激活值检查点（与 megatron-core 配合，减少显存占用）
  activation_checkpointing:
    enabled: True
    partition_activations: True # 与 TP 兼容，激活值按 TP 分片存储
    contiguous_memory_optimization: True

  tensorboard:
    enabled: True
    output_path: ./ds_tensorboard_logs/
    job_name: agent_search_tensorboard

  # 日志与 checkpoint 配置
  steps_per_print: 10
  checkpoint:
    writer:
      type: python
      data_parallel: REPLICA
    use_node_local_storage: True
    path: ./checkpoints

qwen_model_path: Qwen/Qwen2.5-7B # qwen模型hf id

megatron:
  master_addr: localhost
  master_port: 6000
  tensor_model_parallel_size: 2
  pipeline_model_parallel_size: 2
#  micro_batch_size: 64
  data_parallel_size: 2
#  rank: 0
#  local_rank: 0
#  world_size: 2
  num_layers_per_virtual_pipeline_stage: null # vpp will hang. need debug.
#  sequence_parallel: True
  seed: 1

do_search: True   # online search
max_turns: 4

actor:
  strategy: megatron  # This is for backward-compatibility
  ppo_mini_batch_size: ${actor.ppo_micro_batch_size}  # for update
  ppo_micro_batch_size: 8  # for search
  clip_ratio: 0.2
  entropy_coeff: 0.001
  ppo_epochs: 1
  shuffle: True
  load_weight: True
  kl_ctrl:
    type: fixed
    kl_coef: 0.001
  optimizer:
    lr: 2e-7
    min_lr: 1e-8   # only useful for warmup with cosine
    lr_decay_steps: 500
    lr_decay_style: cosine  # select from constant/cosine
    clip_grad: 1.0
    lr_warmup_fraction: 0.1  # the total steps will be injected during runtime
    weight_decay_incr_style: linear  # select from constant/cosine
    start_weight_decay: 1e-8
    end_weight_decay: 1e-7
    total_training_steps: 1005  # must be override by program
    use_checkpoint_opt_param_scheduler: True
    override_opt_param_scheduler: False

critic:
  load_weight: True
  ppo_mini_batch_size: ${actor.ppo_mini_batch_size}
  ppo_micro_batch_size: ${actor.ppo_micro_batch_size}
  ppo_epochs: ${actor.ppo_epochs}
  shuffle: ${actor.shuffle}
  cliprange_value: 0.5
  kl_ctrl:
    type: fixed
    kl_coef: 0.001
  optimizer:
    lr: 1e-5
    min_lr: 1e-6   # only useful for warmup with cosine
    lr_decay_steps: 500
    lr_decay_style: cosine  # select from constant/cosine
    clip_grad: 1.0
    lr_warmup_fraction: 0.015  # the total steps will be injected during runtime
    weight_decay_incr_style: linear  # select from constant/cosine
    start_weight_decay: 0.
    end_weight_decay: 1e-6
    total_training_steps: 1000  # must be override by program
    use_checkpoint_opt_param_scheduler: True
    override_opt_param_scheduler: False

rollout:
  max_new_token: 500
  temperature: 1.0
  top_k: -1

retriever:
  url: http://127.0.0.1:8000/retrieve
  topk: 3

trainer:
  total_epochs: 3
  total_training_steps: null
  project_name: verl_examples
  experiment_name: nq_search_r1
#  logger: ['console', 'wandb']
  logger: ['console']
  nnodes: 1
  n_gpus_per_node: 1
  save_freq: 3
  test_freq: 3
  critic_warmup: 0
  log_interval: 3

algorithm:
  gamma: 1.0
  lam: 1.0
  adv_estimator: gae
  kl_penalty: kl  # how to estimate kl divergence
  kl_ctrl:
    type: fixed
    kl_coef: 0.001
  no_think_rl: False