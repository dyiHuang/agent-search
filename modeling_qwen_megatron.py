from functools import partial
from typing import Optional, Union, List, Dict, Callable

import torch
from torch import Tensor
import torch.nn as nn
from megatron.core.fusions.fused_bias_dropout import get_bias_dropout_add
from megatron.core import parallel_state, tensor_parallel, pipeline_parallel
from megatron.core.models.huggingface import qwen_model
from megatron.core.process_groups_config import ProcessGroupCollection
from megatron.core.transformer import MegatronModule
from megatron.core.transformer.transformer_layer import TransformerConfig, TransformerLayer, TransformerLayerSubmodules
from megatron.core.transformer.attention import SelfAttention, SelfAttentionSubmodules
from megatron.core.transformer.dot_product_attention import DotProductAttention
from megatron.core.transformer.mlp import MLP, MLPSubmodules
from megatron.core.transformer.spec_utils import ModuleSpec
from megatron.core.transformer.identity_op import IdentityOp
from megatron.core.transformer.enums import AttnMaskType
from megatron.core.models.common.embeddings.rotary_pos_embedding import RotaryEmbedding
from megatron.core.distributed.distributed_data_parallel_config import DistributedDataParallelConfig
from megatron.core.enums import ModelType
from peft import LoraConfig, get_peft_model
from transformers import Qwen2Config
from transformers.integrations import use_kernel_forward_from_hub
from transformers.modeling_utils import PreTrainedModel
from transformers import Qwen2ForCausalLM
from transformers.models.qwen2.modeling_qwen2 import Qwen2RotaryEmbedding, eager_attention_forward
from transformers.activations import ACT2FN
from transformers.modeling_utils import ALL_ATTENTION_FUNCTIONS

from transformers.masking_utils import create_causal_mask, create_sliding_window_causal_mask
from transformers.cache_utils import Cache, DynamicCache
from megatron.core.packed_seq_params import PackedSeqParams

from utils import utils, torch_functional
from tensor_parallel import vocab_parallel_log_probs_from_logits, vocab_parallel_compute_entropy_loss

import core_algos
import qwen_load
from tensordict import TensorDict


class Qwen2DotProductAttention(DotProductAttention):
    def __init__(self, config: TransformerConfig, layer_number: int, attn_mask_type: AttnMaskType, attention_type: str,
                 attention_dropout: float = None, softmax_scale: float = None, cp_comm_type: str = None,
                 pg_collection: ProcessGroupCollection = None):
        super().__init__(config, layer_number, attn_mask_type, attention_type, attention_dropout, softmax_scale,
                         cp_comm_type, pg_collection)

        self.num_key_value_groups = self.num_attention_heads_per_partition // self.num_query_groups_per_partition

    def forward(self,
                query: Tensor,
                key: Tensor,
                value: Tensor,
                attention_mask: Tensor,
                attn_mask_type: AttnMaskType = None,
                attention_bias: Tensor = None,
                packed_seq_params: Optional[PackedSeqParams] = None, ):
        attention_interface: Callable = eager_attention_forward
        if self.qwen_config._attn_implementation != "eager":
            attention_interface = ALL_ATTENTION_FUNCTIONS[self.qwen_config._attn_implementation]
        # [sq, b, np, hn],[sq, b, gp, hn] -> [b, np, sq, hn],[b, gp, sq, hn]
        query = query.transpose(0, 1).transpose(1, 2).contiguous()
        key = key.transpose(0, 1).transpose(1, 2).contiguous()
        value = value.transpose(0, 1).transpose(1, 2).contiguous()

        attn_output, attn_weights = attention_interface(
            self,
            query,
            key,
            value,
            attention_mask,
            dropout=self.config.attention_dropout,
            scaling=self.softmax_scale,
            sliding_window=None,  # main diff with Llama
            # **kwargs,
        )

        # =========================
        # Context layer. [sq, b, hp]
        # =========================

        # value -> context layer.
        # [sk, b, np, hn] --> [b, np, sq, hn]

        # context layer shape: [b, np, sq, hn]
        output_size = (value.size(0), query.size(1), query.size(2), value.size(3))

        # change view [b, np, sq, hn]
        context = attn_output.view(*output_size)

        # [b, np, sq, hn] --> [sq, b, np, hn]
        context = context.permute(2, 0, 1, 3).contiguous()

        # [sq, b, np, hn] --> [sq, b, hp]
        new_context_shape = context.size()[:-2] + (self.hidden_size_per_partition,)
        context = context.view(*new_context_shape)

        return context


class Qwen2MegatronAttention(SelfAttention):
    def __init__(self,
                 config: TransformerConfig,
                 layer_number: int,
                 attn_mask_type=AttnMaskType.padding,
                 cp_comm_type: str = None,
                 pg_collection: ProcessGroupCollection = None
                 ):
        config.add_qkv_bias = True
        super().__init__(
            config,
            submodules=SelfAttentionSubmodules(
                linear_qkv=tensor_parallel.ColumnParallelLinear,
                core_attention=Qwen2DotProductAttention,
                linear_proj=tensor_parallel.RowParallelLinear,
                q_layernorm=IdentityOp,
                k_layernorm=IdentityOp,
            ),
            layer_number=layer_number,
            attn_mask_type=attn_mask_type,
            cp_comm_type=cp_comm_type,
            pg_collection=pg_collection,
        )


class Qwen2MegatronMLP(MLP):
    def __init__(self, config: TransformerConfig):
        self.config = config
        mlp = MLPSubmodules(
            linear_fc1=tensor_parallel.ColumnParallelLinear,  # Qwen2.5 gate_proj, up_proj
            linear_fc2=tensor_parallel.RowParallelLinear,  # Qwen2.5 down_proj
        )
        super().__init__(config, submodules=mlp)
        # # Qwen2.5 ç”¨SwiGLUï¼Œ æ›¿æ¢ Megatron é»˜è®¤ GELU
        # self.activation_func = nn.SiLU  # SwiLU = SilU + ç‚¹ç§¯


@use_kernel_forward_from_hub("RMSNorm")
class Qwen2RMSNorm(nn.Module):
    def __init__(self, config: TransformerConfig, hidden_size, eps: float = 1e-6) -> None:
        """
        Qwen2RMSNorm is equivalent to T5LayerNorm
        """
        super().__init__()
        self.weight = nn.Parameter(torch.ones(hidden_size, dtype=config.params_dtype))
        self.variance_epsilon = eps

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        input_dtype = hidden_states.dtype
        hidden_states = hidden_states.to(torch.float32)
        variance = hidden_states.pow(2).mean(-1, keepdim=True)
        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)
        return self.weight * hidden_states.to(input_dtype)

    def extra_repr(self):
        return f"{tuple(self.weight.shape)}, eps={self.variance_epsilon}"


class Qwen2MegatronTransformerLayer(TransformerLayer):
    def __init__(self, config: TransformerConfig, layer_number: int = 1, qwen_config: Qwen2Config = None):
        super().__init__(
            config,
            submodules=TransformerLayerSubmodules(
                input_layernorm=Qwen2RMSNorm,
                self_attention=ModuleSpec(
                    module=Qwen2MegatronAttention,
                    params={"attn_mask_type": AttnMaskType.causal},
                ),
                self_attn_bda=get_bias_dropout_add,
                pre_mlp_layernorm=Qwen2RMSNorm,
                mlp=Qwen2MegatronMLP,
                mlp_bda=get_bias_dropout_add,
                sharded_state_dict_keys_map={
                    #"input_layernorm.": "self_attention.linear_qkv.layer_norm_",
                    #"pre_mlp_layernorm.": "mlp.linear_fc1.layer_norm_"
                },
            ),
            layer_number=layer_number)
        qwen_config._attn_implementation = 'sdpa'
        self.self_attention.core_attention.qwen_config = qwen_config
        self.attention_type = qwen_config.layer_types[layer_number-1]


class Qwen2PreTrainedModel(PreTrainedModel):
    config: Qwen2Config
    base_model_prefix = "model"
    # supports_gradient_checkpointing = True
    # _no_split_modules = ["Qwen2DecoderLayer"]
    # _skip_keys_device_placement = ["past_key_values"]
    # _supports_flash_attn = True
    # _supports_sdpa = True
    # _supports_flex_attn = True

    # _can_compile_fullgraph = True
    # _supports_attention_backend = True
    # _can_record_outputs = {
    #     "hidden_states": Qwen2DecoderLayer,
    #     "attentions": Qwen2Attention,
    # }


class Qwen2MegatronModel(MegatronModule):
    """Megatron å¹¶è¡ŒåŒ–çš„ Qwen2.5-3B æ¨¡å‹"""

    def __init__(self, g_config, qwen_config: Qwen2Config, megatron_config: TransformerConfig):
        super().__init__(megatron_config)
        self.ddp_config = DistributedDataParallelConfig(use_megatron_fsdp=False)
        self.vocab_size = qwen_config.vocab_size
        self.hidden_size = qwen_config.hidden_size
        self.num_layers = qwen_config.num_hidden_layers
        self.tp_size = megatron_config.tensor_model_parallel_size
        self.micro_batch_size = g_config.actor.ppo_micro_batch_size
        self.pp_size = parallel_state.get_pipeline_model_parallel_world_size()  # è·å– PP æ€»è¿›ç¨‹æ•°
        self.pp_rank = parallel_state.get_pipeline_model_parallel_rank()  # å½“å‰ PP stage ç¼–å·ï¼ˆ0~pp_size-1ï¼‰

        # -------------------------- 1. PP æ‹†åˆ†ï¼šæŒ‰ stage åˆ†é…å±‚æ•° --------------------------
        # è®¡ç®—å½“å‰ stage è´Ÿè´£çš„ Transformer å±‚æ•°ï¼ˆå‡åŒ€åˆ†é…ï¼Œæ”¯æŒä½™æ•°å¤„ç†ï¼‰
        self.num_layers_per_stage = self.num_layers // self.pp_size
        self.start_layer = self.pp_rank * self.num_layers_per_stage
        self.end_layer = self.start_layer + self.num_layers_per_stage
        # å¤„ç†ä½™æ•°ï¼ˆæœ€åä¸€ä¸ª stage å¤šæ‰¿æ‹…å‰©ä½™å±‚æ•°ï¼‰
        if self.pp_rank == self.pp_size - 1:
            self.end_layer = self.num_layers

        # -------------------------- 2. ä»…å½“å‰ stage åˆå§‹åŒ–è‡ªå·±è´Ÿè´£çš„å±‚ --------------------------
        # åµŒå…¥å±‚ï¼šä»… PP stage 0 åˆå§‹åŒ–ï¼ˆè¾“å…¥å±‚å¿…é¡»åœ¨ç¬¬ä¸€ä¸ª stageï¼‰
        # åµŒå…¥å±‚ï¼ˆTensor Parallelï¼‰
        self.embedding = None
        if self.pp_rank == 0:
            self.embedding = tensor_parallel.VocabParallelEmbedding(
                self.vocab_size, self.hidden_size, init_method=torch.nn.init.xavier_uniform_, config=megatron_config
            )

        # Rotary Embeddingï¼šä»… PP stage 0 è®¡ç®—ï¼ˆåç»­ stage å¤ç”¨æˆ–ä¼ é€’ï¼‰
        # Rotary Embedding (Qwen2.5æ ‡å‡†å®ç°)
        # self.rotary_emb = RotaryEmbedding(
        #     megatron_config.kv_channels, rotary_percent=1.0, seq_len_interpolation_factor=1.0
        # )
        self.rotary_emb = Qwen2RotaryEmbedding(config=qwen_config)

        # Transformer å±‚ï¼šä»…åˆå§‹åŒ–å½“å‰ stage è´Ÿè´£çš„å±‚ï¼ˆæ ¸å¿ƒ PP æ‹†åˆ†ï¼‰
        self.layers = nn.ModuleList([
            Qwen2MegatronTransformerLayer(megatron_config, i - self.start_layer + 1, qwen_config)
            for i in range(self.start_layer, self.end_layer)
        ])

        # è¾“å‡ºå±‚ï¼ˆfinal_norm + lm_headï¼‰ï¼šä»… PP æœ€åä¸€ä¸ª stage åˆå§‹åŒ–
        self.final_norm = None
        self.lm_head = None
        utils.print_rank_0(f"self.pp_rank: {self.pp_rank}, self.pp_size: {self.pp_size}")
        if self.pp_rank == self.pp_size - 1:
            self.final_norm = Qwen2RMSNorm(megatron_config, self.hidden_size, eps=qwen_config.rms_norm_eps)
            self.lm_head = tensor_parallel.ColumnParallelLinear(
                self.hidden_size, self.vocab_size, config=megatron_config, bias=False,
                init_method=megatron_config.init_method
            )
        self.model_type = ModelType.encoder_or_decoder

    def set_input_tensor(self, input_tensor):
        """Set input tensor to be used instead of forward()'s input.

        When doing pipeline parallelism the input from the previous
        stage comes from communication, not from the input, so the
        model's forward_step_func won't have it. This function is thus
        used by internal code to bypass the input provided by the
        forward_step_func"""
        self.input_tensor = input_tensor

    def forward(self, input_ids: torch.Tensor, attention_mask: torch.Tensor = None, only_last_token: bool = False):
        """é€‚é… PP+TP å¹¶è¡Œçš„å‰å‘ä¼ æ’­ï¼šå¤„ç†è·¨ stage æ•°æ®æµè½¬"""

        # -------------------------- 1. ç¬¬ä¸€ä¸ª PP stageï¼šæ‰§è¡ŒåµŒå…¥å±‚ + Rotary ç¼–ç  --------------------------
        # åµŒå…¥ + Rotary ç¼–ç 
        hidden_states = None
        rotary_pos_emb = None
        if self.pp_rank == 0:
            ori_input_ids = input_ids
            # [b, s, h] -> [s, b, h]
            input_ids = input_ids.transpose(1, 0).contiguous()
            # åµŒå…¥å±‚ï¼ˆä»… stage 0 æœ‰ï¼‰
            hidden_states = self.embedding(input_ids)

            ori_input_ids.transpose(1, 0)

            # æå‰å¤„ç† only_last_tokenï¼ˆå‡å°‘è·¨ stage é€šä¿¡é‡ï¼‰
            # if only_last_token:
            #     hidden_states = hidden_states[:, -1:]  # [batch, 1, hidden_size/tp_size]
            #     rotary_pos_emb = rotary_pos_emb[:, -1:] if isinstance(rotary_pos_emb, torch.Tensor) else None
        else:
            # self.hidden_states should be passed by Megatron
            hidden_states = self.input_tensor

        # -------------------------- ä¿®æ­£æ³¨æ„åŠ›æ©ç ç»´åº¦ --------------------------
        # if attention_mask is not None:
        #     # è‡ªæ³¨æ„åŠ›éœ€è¦çš„æ©ç å½¢çŠ¶ï¼š[batch_size, 1, seq_len, seq_len]
        #     # 1. å°† [batch_size, seq_len] æ‰©å±•ä¸º [batch_size, 1, 1, seq_len]
        #     attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)

        # æ£€æŸ¥Rotary Embedding
        use_cache: Optional[bool] = None
        cache_position: Optional[torch.LongTensor] = None
        past_key_values: Optional[Cache] = None
        position_ids: Optional[torch.LongTensor] = None
        if use_cache and past_key_values is None:
            past_key_values = DynamicCache(config=self.model.config)

        if cache_position is None:
            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
            cache_position = torch.arange(
                past_seen_tokens, past_seen_tokens + hidden_states.shape[0], device=hidden_states.device
            )

        if position_ids is None:
            position_ids = cache_position.unsqueeze(0)

        # It may already have been prepared by e.g. `generate`
        if not isinstance(causal_mask_mapping := attention_mask, dict):
            hidden_states = hidden_states.transpose(1, 0)
            # Prepare mask arguments
            mask_kwargs = {
                "config": self.model.config,
                "input_embeds": hidden_states,
                "attention_mask": attention_mask,
                "cache_position": cache_position,
                "past_key_values": past_key_values,
                "position_ids": position_ids,
            }
            # Create the masks
            causal_mask_mapping = {
                "full_attention": create_causal_mask(**mask_kwargs),
            }
            hidden_states = hidden_states.transpose(1, 0).contiguous()

        # seq_len = hidden_states.size(0)
        # position_ids = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0)
        # è®¡ç®— Rotary åµŒå…¥ï¼ˆä»… stage 0 è®¡ç®—ï¼Œä¼ é€’ç»™åç»­ stageï¼‰
        cos, sin = self.rotary_emb(hidden_states, position_ids)  # [b, s, h]
        cos_sin = torch.cat([cos, sin], dim=-1).transpose(1, 0).contiguous()

        rotary_pos_emb = cos_sin, cos_sin

        # -------------------------- 2. å½“å‰ stage å¤„ç†è‡ªå·±çš„ Transformer å±‚ --------------------------
        for layer in self.layers:
            hidden_states = layer(
                hidden_states, attention_mask=causal_mask_mapping[layer.attention_type], rotary_pos_emb=rotary_pos_emb
            )
            # å–å…ƒç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºæ–°çš„ hidden_statesï¼ˆå¿½ç•¥ context ä¿¡æ¯ï¼‰
            hidden_states = hidden_states[0] if isinstance(hidden_states, tuple) else hidden_states
            # PP ä¸‹æ— éœ€åœ¨å±‚å¾ªç¯ä¸­å¤„ç† only_last_tokenï¼ˆå·²åœ¨ stage 0 å¤„ç†æˆ–åç»­ stage ä¿æŒï¼‰
            # if only_last_token:
            #     # ä»…ä¿ç•™æœ€åä¸€ä¸ªtokençš„hidden statesï¼Œå‡å°‘åç»­è®¡ç®—é‡
            #     hidden_states = hidden_states[:, -1:]  # [batch, 1, hidden_size]
            #     # å¯¹åº”çš„rotary_pos_embä¹Ÿä»…ä¿ç•™æœ€åä¸€ä¸ªä½ç½®ï¼ˆè‹¥å±‚å†…ä¾èµ–ï¼‰
            #     rotary_pos_emb = rotary_pos_emb[:, -1:] if isinstance(rotary_pos_emb, torch.Tensor) else None

        # è¾“å‡º
        if self.pp_rank == self.pp_size - 1:
            hidden_states = self.final_norm(hidden_states)
            logits = self.lm_head(hidden_states)
            # [s, b, h] -> [b, s, h]
            logits = logits[0].transpose(1, 0).contiguous()

            # è‹¥ä»…éœ€æœ€åä¸€ä¸ªtokençš„logitsï¼Œç›´æ¥è¿”å›
            if only_last_token:
                logits = logits[:, -1:]  # [batch, 1, vocab_size/tp_size]

            logits = logits.float()
            return logits
        return hidden_states

    @torch.no_grad()  # ç”Ÿæˆè¿‡ç¨‹ç¦ç”¨æ¢¯åº¦è®¡ç®—
    def generate(
            self,
            input_ids: torch.Tensor,
            max_length: int = 512,
            eos_token_id: Optional[int] = None,
            temperature: float = 1.0,
            top_k: Optional[int] = None,
            attention_mask: Optional[torch.Tensor] = None,
            pad_token_id: Optional[int] = None,
    ) -> torch.Tensor:
        """
        è‡ªå›å½’ç”Ÿæˆæ–¹æ³•ï¼ˆé€‚é… Megatron å¹¶è¡Œé€»è¾‘ï¼‰
        Args:
            input_ids: è¾“å…¥promptçš„token idï¼Œshape: [batch_size, seq_len]
            max_length: ç”Ÿæˆçš„æœ€å¤§æ€»é•¿åº¦ï¼ˆå«promptï¼‰
            eos_token_id: ç»ˆæ­¢ç¬¦token idï¼ˆQwen2.5é»˜è®¤ï¼š151643ï¼‰
            temperature: é‡‡æ ·æ¸©åº¦ï¼ˆ<1.0æ›´ç¡®å®šï¼Œ>1.0æ›´å¤šæ ·ï¼‰
            top_k: ä»…ä»top-kä¸ªé«˜æ¦‚ç‡tokenä¸­é‡‡æ ·ï¼ˆNoneè¡¨ç¤ºä¸é™åˆ¶ï¼‰
            attention_mask: è¾“å…¥promptçš„attention maskï¼Œshape: [batch_size, seq_len]
            pad_token_id: padding token idï¼ˆç”¨äºå¡«å……batchä¸­æå‰ç»ˆæ­¢çš„æ ·æœ¬ï¼‰
        Returns:
            ç”Ÿæˆçš„å®Œæ•´token idï¼Œshape: [batch_size, max_length]
        """
        # 1. åˆå§‹åŒ–é…ç½®å’Œè®¾å¤‡
        self.eval()  # ç”Ÿæˆæ—¶åˆ‡æ¢ä¸ºè¯„ä¼°æ¨¡å¼ï¼ˆç¦ç”¨dropoutï¼‰
        batch_size, current_seq_len = input_ids.shape
        device = input_ids.device
        eos_token_id = eos_token_id or 151643  # Qwen2.5é»˜è®¤eos_token_id
        pad_token_id = pad_token_id or eos_token_id  # é»˜è®¤ä¸ºeos_token_id

        # 2. åˆå§‹åŒ–attention maskï¼ˆè‹¥æœªæä¾›ï¼‰
        if attention_mask is None:
            attention_mask = torch.ones_like(input_ids, dtype=torch.bool, device=device)
        else:
            attention_mask = attention_mask.bool()  # ç¡®ä¿æ˜¯boolç±»å‹

        # 3. æ£€æŸ¥æœ€å¤§é•¿åº¦åˆæ³•æ€§
        if max_length <= current_seq_len:
            return input_ids  # è‹¥å·²è¾¾åˆ°æœ€å¤§é•¿åº¦ï¼Œç›´æ¥è¿”å›

        # 4. åˆå§‹åŒ–ç”ŸæˆçŠ¶æ€ï¼šè®°å½•æ¯ä¸ªæ ·æœ¬æ˜¯å¦å·²ç”Ÿæˆeos
        finished_mask = torch.zeros(batch_size, dtype=torch.bool, device=device)  # [batch_size]

        # 5. é¢„åˆ†é…ç”Ÿæˆç»“æœçš„tensorï¼ˆé¿å…åŠ¨æ€æ‹¼æ¥ï¼‰
        generated_ids = torch.full(
            (batch_size, max_length), pad_token_id, dtype=torch.long, device=device
        )
        generated_ids[:, :current_seq_len] = input_ids  # å¡«å……promptéƒ¨åˆ†

        # 6. è‡ªå›å½’ç”Ÿæˆå¾ªç¯
        for step in range(current_seq_len, max_length):
            # a. è·å–å½“å‰è¾“å…¥ï¼ˆprompt + å·²ç”Ÿæˆçš„tokenï¼‰
            current_input_ids = generated_ids[:, :step]  # [batch_size, step]
            current_attention_mask = attention_mask[:, :step]  # [batch_size, step]

            batches = TensorDict(
                source={
                    "input_ids": current_input_ids,
                    "attention_mask": current_attention_mask
                },
                batch_size=batch_size)
            # b. å‰å‘ä¼ æ’­ï¼šä»…è®¡ç®—æœ€åä¸€ä¸ªtokençš„logitsï¼ˆæå‡æ•ˆç‡ï¼‰
            logits = self.forward_backward_batch(
                batch=batches,
                only_last_token=True,  # å…³é”®ä¼˜åŒ–ï¼šä»…è¿”å›æœ€åä¸€ä¸ªtokençš„logits
                forward_only=True,
            )  # PP+TPä¸‹ï¼šLIST[batch_size/pp_size, 1, vocab_size/tp_size]

            # logits = [
            #     # c. å¼ é‡å¹¶è¡Œèšåˆï¼šæ”¶é›†æ‰€æœ‰TPè¿›ç¨‹çš„logitsï¼Œå¾—åˆ°å®Œæ•´vocabåˆ†å¸ƒ
            #     self.gather_logits_across_tp(l) for l in logits]  # [batch_size, 1, vocab_size]

            if parallel_state.is_pipeline_last_stage(ignore_virtual=True):
                # ç¡®ä¿æ­£ç¡®èšåˆæ‰€æœ‰micro batchçš„logits
                if isinstance(logits, list):
                    logits = torch.cat(logits, dim=0)  # (batch_size, 1, vocab_size/tp_size)
                logits = logits.to(torch.float32)
            else:
                logits = torch.empty(size=(batch_size, 1, self.vocab_size),
                                     dtype=torch.float32,
                                     device=input_ids.device)

            # utils.print_rank_0(f"before broadcast, logits shape : {logits.shape}")
            # broadcast across pp ranks
            torch.distributed.broadcast(tensor=logits,
                                        src=parallel_state.get_pipeline_model_parallel_last_rank(),
                                        group=parallel_state.get_pipeline_model_parallel_group(),
                                        async_op=False)

            # utils.print_rank_0(f"after broadcast, logits shape : {logits.shape}")

            # d. åº”ç”¨æ¸©åº¦é‡‡æ ·å’ŒTop-Kè¿‡æ»¤
            logits = logits / temperature  # æ¸©åº¦è°ƒæ•´
            if top_k is not None and top_k > 0:
                logits = self._top_k_filter(logits, top_k)  # Top-Kè¿‡æ»¤

            # utils.print_rank_0(f"after top_k, logits shape : {logits.shape}")
            # e. é‡‡æ ·å¾—åˆ°ä¸‹ä¸€ä¸ªtokenï¼ˆgreedyæˆ–éšæœºé‡‡æ ·ï¼‰
            next_token_log_probs = torch.softmax(logits, dim=-1)  # [batch_size, 1, vocab_size]
            next_token = torch.multinomial(next_token_log_probs.squeeze(1), num_samples=1)  # [batch_size, 1]

            # f. å¤„ç†åœæ­¢æ¡ä»¶ï¼šæ ‡è®°å·²ç”Ÿæˆeosçš„æ ·æœ¬
            finished_mask = finished_mask | (next_token.squeeze(1) == eos_token_id)
            # å¯¹å·²å®Œæˆçš„æ ·æœ¬ï¼Œå¡«å……pad_token_id
            next_token = torch.where(finished_mask.unsqueeze(1), torch.tensor(pad_token_id, device=device), next_token)

            # g. ä¿å­˜ç”Ÿæˆçš„token
            generated_ids[:, step] = next_token.squeeze(1)

            # h. æ›´æ–°attention_maskï¼ˆæ–°å¢tokençš„maskä¸ºTrueï¼‰
            new_attention_mask = torch.ones((batch_size, 1), dtype=torch.bool, device=device)
            attention_mask = torch.cat([attention_mask, new_attention_mask], dim=-1)  # [batch_size, step+1]

            # i. è‹¥æ‰€æœ‰æ ·æœ¬éƒ½å·²å®Œæˆï¼Œæå‰é€€å‡º
            if finished_mask.all():
                break

        return generated_ids

    def forward_backward_batch(self, batch: TensorDict, only_last_token=False,
                               forward_only=False, post_process_fn=None, meta_info: Dict = None):
        """
        We assume:
        - The model takes input: (input_ids, attention_mask, position_ids). No rmpad for the input
        - The communication shape is (seq_len , micro_batch_size, hidden_size)
        """
        # broadcast from last pp rank to all other pp ranks
        # TODO: actually, we just need to control the sampling order.
        batch = batch.contiguous()
        torch_functional.broadcast_dict_tensor(
            batch,
            src=parallel_state.get_pipeline_model_parallel_last_rank(),
            group=parallel_state.get_pipeline_model_parallel_group())

        batches = self.split_dict_tensor_into_batches(batch, batch_size=self.micro_batch_size)
        # compute input shapes for pp stages
        # input_shapes = self.compute_transformers_input_shapes(
        #     batches,
        #     attention_mask,
        #     meta_info={
        #         'hidden_size': self.hidden_size
        #     })
        n_micro_batch = len(batches)
        seq_len = batches[0]["input_ids"].shape[1]

        forward_backward_func = pipeline_parallel.get_forward_backward_func()

        def loss_func(output, data):
            if forward_only:
                if post_process_fn is None:
                    output = self.gather_logits_across_tp(output)
                    return 1.0, output
                else:
                    return 1.0, post_process_fn(output, data)
            responses = data['responses']
            response_length = responses.size(1)
            attention_mask = data['attention_mask']
            response_mask = attention_mask[:, -response_length:]
            ref_log_probs = data['ref_log_probs']
            advantages = data['advantages']

            clip_ratio = 0.2
            entropy_coeff = 0.1
            if not meta_info:
                clip_ratio = meta_info['clip_ratio']
                entropy_coeff = meta_info['entropy_coeff']

            # compute policy loss
            logits = output
            logits = logits[:, -response_length - 1:-1].contiguous()
            log_prob = vocab_parallel_log_probs_from_logits(logits, responses)
            pg_loss, pg_clipfrac, ppo_kl = core_algos.compute_policy_loss(old_log_prob=ref_log_probs,
                                                                          log_prob=log_prob,
                                                                          advantages=advantages,
                                                                          eos_mask=response_mask,
                                                                          cliprange=clip_ratio)
            entropy_loss = vocab_parallel_compute_entropy_loss(logits, eos_mask=response_mask)
            policy_loss = pg_loss - entropy_loss * entropy_coeff
            # return loss and stats
            stats = {
                'actor/entropy_loss': entropy_loss.detach().item(),
                'actor/pg_loss': pg_loss.detach().item(),
                'actor/pg_clipfrac': pg_clipfrac.detach().item(),
                'actor/ppo_kl': ppo_kl.detach().item()
            }
            return policy_loss, stats

        def forward_step(batch_iter, model):
            micro_batch = next(batch_iter)
            output = model(input_ids=micro_batch["input_ids"], attention_mask=None,
                           # attention_mask=micro_batch["attention_mask"],
                           only_last_token=only_last_token)
            return output, partial(loss_func, data=micro_batch)

        # batch should be a list of batches inside micro-batches
        batch_generator = self.make_batch_generator(batches, vpp_size=1)

        # for flash-attn: (seq_len, batch_size, hidden_size) = (mbs*seq_len, 1, hidden_size)
        if parallel_state.get_pipeline_model_parallel_world_size() > 1:
            losses_reduced = forward_backward_func(
                forward_step_func=forward_step,
                data_iterator=batch_generator,
                model=self,
                num_microbatches=n_micro_batch,
                seq_length=seq_len,
                # hidden_size=self.model_config.hidden_size,
                micro_batch_size=self.micro_batch_size,
                forward_only=forward_only,
            )
        else:
            losses_reduced = forward_backward_func(
                forward_step_func=forward_step,
                data_iterator=batch_generator,
                model=self,
                num_microbatches=n_micro_batch,
                seq_length=seq_len,
                # hidden_size=self.model_config.hidden_size,
                micro_batch_size=self.micro_batch_size,
                forward_only=forward_only,
            )
        # loss_reduces contains the stats returned from loss_func
        return losses_reduced

    @staticmethod
    def make_batch_generator(batches, vpp_size):
        if vpp_size > 1:
            # has vpp
            batch_generator = [batches] * vpp_size  # number of vpp chunks
            batch_generator = [iter(b) for b in batch_generator]
        else:
            # no vpp
            batch_generator = iter(batches)
        return batch_generator

    # @staticmethod
    # def compute_transformers_input_shapes(batches, attention_masks, meta_info):
    #     from flash_attn.bert_padding import unpad_input  # flash 2 is a must for Megatron
    #     # pre-compute input shapes for each micro-batch at each pp stage
    #     input_shapes = []
    #     for input_ids, attention_mask in zip(batches, attention_masks):
    #         input_ids_rmpad = unpad_input(input_ids.unsqueeze(dim=-1), attention_mask)[0]  # (total_nnz, 1)
    #         # compute shapes for model_inputs
    #         input_shapes.append(torch.Size([input_ids_rmpad.shape[0], 1, meta_info['hidden_size']]))
    #     return input_shapes

    @staticmethod
    def split_dict_tensor_into_batches(tensors: TensorDict, batch_size) -> List[TensorDict]:
        assert tensors.batch_size[0] % batch_size == 0, \
            f'input data batch size: {tensors.batch_size[0]}, split batch size: {batch_size}'
        return tensors.split(batch_size)

    def gather_logits_across_tp(self, logits: torch.Tensor) -> torch.Tensor:
        """å¼ é‡å¹¶è¡Œï¼ˆTPï¼‰ä¸‹èšåˆlogitsï¼šå°†å„è¿›ç¨‹çš„åˆ†ç‰‡logitsæ‹¼æ¥ä¸ºå®Œæ•´vocabåˆ†å¸ƒ
        Args:
            logits: TPä¸‹çš„åˆ†ç‰‡logitsï¼Œshape: [batch_size, 1, vocab_size/tp_size]
        Returns:
            å®Œæ•´logitsï¼Œshape: [batch_size, 1, vocab_size]
        """
        if self.tp_size == 1:
            return logits  # å•å¡æ— éœ€èšåˆ

        # ä½¿ç”¨Megatronçš„tensor_model_parallel_all_gatherèšåˆï¼ˆè·¨TPè¿›ç¨‹ï¼‰
        gathered_logits = tensor_parallel.gather_from_tensor_model_parallel_region(
            logits,
            group=parallel_state.get_tensor_model_parallel_group()
        )
        return gathered_logits

    @staticmethod
    def _top_k_filter(logits: torch.Tensor, top_k: int) -> torch.Tensor:
        """Top-Kè¿‡æ»¤ï¼šå°†æ¦‚ç‡æ’åä½äºtop-kçš„tokençš„logitsè®¾ä¸º-æ— ç©·ï¼ˆä¸å‚ä¸é‡‡æ ·ï¼‰"""
        if top_k >= logits.size(-1):
            return logits  # è‹¥top-kå¤§äºvocab_sizeï¼Œæ— éœ€è¿‡æ»¤

        # è·å–top-kçš„logitså€¼å’Œå¯¹åº”çš„ç´¢å¼•
        top_k_values, _ = torch.topk(logits, k=top_k, dim=-1)
        min_top_k_value = top_k_values[:, :, -1:]  # [batch_size, 1, 1]
        # è¿‡æ»¤ï¼šä½äºmin_top_k_valueçš„logitsè®¾ä¸º-æ— ç©·
        logits = torch.where(logits >= min_top_k_value, logits, torch.tensor(-float("inf"), device=logits.device))
        return logits

    def detailed_forward_debug(self, input_ids, attention_mask=None):
        """è¯¦ç»†çš„å‰å‘ä¼ æ’­è°ƒè¯•"""
        utils.print_rank_0("=== è¯¦ç»†å‰å‘ä¼ æ’­è°ƒè¯•å¼€å§‹ ===")

        # åŸå§‹è¾“å…¥
        utils.print_rank_0(f"è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
        utils.print_rank_0(f"è¾“å…¥token: {input_ids[0].cpu().numpy()}")

        # åµŒå…¥å±‚
        hidden_states = self.embedding(input_ids.transpose(0, 1))
        utils.print_rank_0(
            f"åµŒå…¥å±‚è¾“å‡º - å½¢çŠ¶: {hidden_states.shape}, å‡å€¼: {hidden_states.mean():.6f}, æ ‡å‡†å·®: {hidden_states.std():.6f}")

        # æ£€æŸ¥Rotary Embedding
        seq_len = hidden_states.size(0)
        position_ids = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0)
        cos, sin = self.rotary_emb(hidden_states, position_ids)
        utils.print_rank_0(f"Rotary cos - å½¢çŠ¶: {cos.shape}, èŒƒå›´: [{cos.min():.3f}, {cos.max():.3f}]")
        utils.print_rank_0(f"Rotary sin - å½¢çŠ¶: {sin.shape}, èŒƒå›´: [{sin.min():.3f}, {sin.max():.3f}]")
        cos_sin = torch.cat([cos, sin], dim=-1).transpose(1, 0).contiguous()

        rotary_pos_emb = cos_sin, cos_sin

        # é€å±‚æ£€æŸ¥Transformer
        for layer_idx, layer in enumerate(self.layers):
            utils.print_rank_0(f"\n--- ç¬¬{layer_idx}å±‚ ---")

            # è¾“å…¥ç»Ÿè®¡
            input_mean = hidden_states.mean().item()
            input_std = hidden_states.std().item()
            utils.print_rank_0(f"è¾“å…¥ - å‡å€¼: {input_mean:.6f}, æ ‡å‡†å·®: {input_std:.6f}")

            # å±‚å‰å‘ä¼ æ’­
            layer_output = layer(hidden_states, attention_mask=attention_mask, rotary_pos_emb=rotary_pos_emb)

            # å¤„ç†è¾“å‡ºï¼ˆå¯èƒ½æ˜¯tupleï¼‰
            if isinstance(layer_output, tuple):
                utils.print_rank_0(f"å±‚è¾“å‡ºæ˜¯tupleï¼Œé•¿åº¦: {len(layer_output)}")
                hidden_states = layer_output[0]
                if len(layer_output) > 1:
                    utils.print_rank_0(
                        f"é¢å¤–è¾“å‡º: {[x.shape if hasattr(x, 'shape') else type(x) for x in layer_output[1:]]}")
            else:
                hidden_states = layer_output

            # è¾“å‡ºç»Ÿè®¡
            output_mean = hidden_states.mean().item()
            output_std = hidden_states.std().item()
            utils.print_rank_0(f"è¾“å‡º - å‡å€¼: {output_mean:.6f}, æ ‡å‡†å·®: {output_std:.6f}")

            # æ£€æŸ¥å˜åŒ–
            change = abs(output_mean - input_mean)
            utils.print_rank_0(f"å‡å€¼å˜åŒ–: {change:.6f}")

            # æ£€æŸ¥NaN/Inf
            if torch.isnan(hidden_states).any():
                utils.print_rank_0(f"âš ï¸  ç¬¬{layer_idx}å±‚è¾“å‡ºåŒ…å«NaN!")
            if torch.isinf(hidden_states).any():
                utils.print_rank_0(f"âš ï¸  ç¬¬{layer_idx}å±‚è¾“å‡ºåŒ…å«Inf!")

            # åªæ£€æŸ¥å‰3å±‚ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
            if layer_idx >= 2:
                utils.print_rank_0("... (è·³è¿‡åç»­å±‚è¯¦ç»†è¾“å‡º)")
                break

        # æœ€ç»ˆå±‚
        if self.final_norm:
            hidden_states = self.final_norm(hidden_states)
            utils.print_rank_0(f"æœ€ç»ˆå½’ä¸€åŒ– - å‡å€¼: {hidden_states.mean():.6f}, æ ‡å‡†å·®: {hidden_states.std():.6f}")

        # LM Head
        hidden_states = self.lm_head(hidden_states)
        if isinstance(hidden_states, tuple):
            utils.print_rank_0(f"lm_headè¾“å‡ºæ˜¯tupleï¼Œé•¿åº¦: {len(hidden_states)}")
            hidden_states = hidden_states[0]
            if len(hidden_states) > 1:
                utils.print_rank_0(
                    f"lm_headé¢å¤–è¾“å‡º: {[x.shape if hasattr(x, 'shape') else type(x) for x in hidden_states[1:]]}")
        else:
            hidden_states = hidden_states
        logits = hidden_states.transpose(0, 1)
        utils.print_rank_0(f"æœ€ç»ˆlogits - å½¢çŠ¶: {logits.shape}, å‡å€¼: {logits.mean():.6f}, æ ‡å‡†å·®: {logits.std():.6f}")

        # æ£€æŸ¥logitsçš„åˆç†æ€§
        top5_values, top5_indices = torch.topk(logits[0, -1], 5)
        utils.print_rank_0(f"æœ€åä¸€ä¸ªtokençš„top-5 logits:")
        for i, (value, idx) in enumerate(zip(top5_values, top5_indices)):
            utils.print_rank_0(f"  {i + 1}. token {idx.item()}: {value.item():.3f}")

        return logits

    def debug_generation_sampling(self, input_ids, num_steps=3):
        """è°ƒè¯•ç”Ÿæˆè¿‡ç¨‹ä¸­çš„é‡‡æ ·"""
        utils.print_rank_0("=== ç”Ÿæˆè¿‡ç¨‹é‡‡æ ·è°ƒè¯• ===")

        current_input = input_ids
        attention_mask = torch.ones_like(input_ids, dtype=torch.bool)

        for step in range(num_steps):
            utils.print_rank_0(f"\n--- ç”Ÿæˆæ­¥éª¤ {step + 1} ---")

            # å‰å‘ä¼ æ’­è·å–logits
            with torch.no_grad():
                logits = self.forward(
                    input_ids=current_input,
                    attention_mask=attention_mask,
                    only_last_token=True  # åªè·å–æœ€åä¸€ä¸ªtokençš„logits
                )

            utils.print_rank_0(f"Logitså½¢çŠ¶: {logits.shape}")
            utils.print_rank_0(f"LogitsèŒƒå›´: [{logits.min():.3f}, {logits.max():.3f}]")

            # æ£€æŸ¥logitsçš„åˆ†å¸ƒ
            last_token_logits = logits[:, -1]  # [batch_size, vocab_size]
            probs = torch.softmax(last_token_logits, dim=-1)

            # é‡‡æ ·å‰çš„æ¦‚ç‡åˆ†å¸ƒ
            top_probs, top_indices = torch.topk(probs[0], 10)
            utils.print_rank_0("Top-10 æ¦‚ç‡åˆ†å¸ƒ:")
            for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):
                token_str = self.tokenizer.decode([idx.item()]) if hasattr(self, 'tokenizer') else str(idx.item())
                utils.print_rank_0(f"  {i + 1}. [{idx.item():6d}] '{token_str}': {prob.item():.4f}")

            # é‡‡æ ·
            next_token = torch.multinomial(probs, num_samples=1)
            utils.print_rank_0(f"é‡‡æ ·çš„ä¸‹ä¸€ä¸ªtoken: {next_token[0].item()}")

            # æ›´æ–°è¾“å…¥
            current_input = torch.cat([current_input, next_token], dim=1)
            attention_mask = torch.cat([attention_mask, torch.ones_like(next_token, dtype=torch.bool)], dim=1)

            utils.print_rank_0(f"æ›´æ–°åè¾“å…¥é•¿åº¦: {current_input.shape[1]}")

        return current_input

    def run_comprehensive_debug(self, tokenizer):
        """è¿è¡Œå…¨é¢çš„è°ƒè¯•"""
        utils.print_rank_0("ğŸš€ å¼€å§‹å…¨é¢è°ƒè¯•...")

        # ä½¿ç”¨å›ºå®šçš„ç®€å•è¾“å…¥
        test_prompt = "Hello"
        if tokenizer is not None:
            input_ids = tokenizer.encode(test_prompt, return_tensors="pt").to('cuda')
        else:
            # å¦‚æœæ²¡æœ‰tokenizerï¼Œä½¿ç”¨ç®€å•æ•°å­—
            input_ids = torch.tensor([[1, 2, 3]], device='cuda')

        utils.print_rank_0(f"æµ‹è¯•è¾“å…¥: '{test_prompt}' -> {input_ids.cpu().numpy()}")

        # 1. è¯¦ç»†å‰å‘ä¼ æ’­
        utils.print_rank_0("\n" + "=" * 50)
        utils.print_rank_0("1. è¯¦ç»†å‰å‘ä¼ æ’­æ£€æŸ¥")
        utils.print_rank_0("=" * 50)
        logits = self.detailed_forward_debug(input_ids)

        # 2. ç”Ÿæˆè¿‡ç¨‹é‡‡æ ·æ£€æŸ¥
        utils.print_rank_0("\n" + "=" * 50)
        utils.print_rank_0("2. ç”Ÿæˆè¿‡ç¨‹é‡‡æ ·æ£€æŸ¥")
        utils.print_rank_0("=" * 50)
        generated = self.debug_generation_sampling(input_ids)

        # 3. éªŒè¯æœ€ç»ˆè¾“å‡º
        if tokenizer is not None:
            generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
            utils.print_rank_0(f"\næœ€ç»ˆç”Ÿæˆæ–‡æœ¬: '{generated_text}'")
        else:
            utils.print_rank_0(f"\næœ€ç»ˆç”Ÿæˆtoken: {generated[0].cpu().numpy()}")

        # è¿è¡Œè¿™äº›è°ƒè¯•å‡½æ•°æ¥å®šä½å…·ä½“é—®é¢˜
        self.debug_attention_mechanism(input_ids)
        self.debug_residual_connections(input_ids)
        self.debug_lm_head_output(input_ids)

        # è¿è¡Œè¿™äº›è°ƒè¯•æ¥ç¡®è®¤é—®é¢˜
        self.debug_mlp_implementation(input_ids)
        self.check_mlp_weights(layer_idx=2)

        utils.print_rank_0("âœ… å…¨é¢è°ƒè¯•å®Œæˆ")

    def debug_attention_mechanism(self, input_ids):
        """è°ƒè¯•æ³¨æ„åŠ›æœºåˆ¶"""
        utils.print_rank_0("=== æ³¨æ„åŠ›æœºåˆ¶è°ƒè¯• ===")

        hidden_states = self.embedding(input_ids.transpose(0, 1))
        attention_mask = torch.ones_like(input_ids, dtype=torch.bool)
        # æ£€æŸ¥Rotary Embedding
        seq_len = hidden_states.size(0)
        position_ids = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0)
        cos, sin = self.rotary_emb(hidden_states, position_ids)
        utils.print_rank_0(f"Rotary cos - å½¢çŠ¶: {cos.shape}, èŒƒå›´: [{cos.min():.3f}, {cos.max():.3f}]")
        utils.print_rank_0(f"Rotary sin - å½¢çŠ¶: {sin.shape}, èŒƒå›´: [{sin.min():.3f}, {sin.max():.3f}]")
        cos_sin = torch.cat([cos, sin], dim=-1).transpose(1, 0).contiguous()

        rotary_pos_emb = cos_sin, cos_sin

        for layer_idx in range(3):  # åªæ£€æŸ¥å‰3å±‚
            utils.print_rank_0(f"\n--- ç¬¬{layer_idx}å±‚æ³¨æ„åŠ›è°ƒè¯• ---")
            layer = self.layers[layer_idx]

            # æ£€æŸ¥è¾“å…¥
            input_stats = f"è¾“å…¥: mean={hidden_states.mean():.6f}, std={hidden_states.std():.6f}"

            # æ³¨æ„åŠ›å±‚å‰å‘ä¼ æ’­
            attention_output = layer.self_attention(
                hidden_states,
                attention_mask=attention_mask,
                rotary_pos_emb=rotary_pos_emb
            )

            if isinstance(attention_output, tuple):
                attention_output = attention_output[0]

            utils.print_rank_0(
                f"{input_stats} -> æ³¨æ„åŠ›è¾“å‡º: mean={attention_output.mean():.6f}, std={attention_output.std():.6f}")

            # æ£€æŸ¥æ³¨æ„åŠ›æƒé‡
            if hasattr(layer.self_attention.core_attention, 'attention_scores'):
                scores = layer.self_attention.core_attention.attention_scores
                utils.print_rank_0(f"æ³¨æ„åŠ›åˆ†æ•°èŒƒå›´: [{scores.min():.3f}, {scores.max():.3f}]")

            # ç»§ç»­å®Œæ•´å±‚çš„å‰å‘ä¼ æ’­
            layer_output = layer(hidden_states)
            if isinstance(layer_output, tuple):
                hidden_states = layer_output[0]
            else:
                hidden_states = layer_output

    def debug_residual_connections(self, input_ids):
        """è°ƒè¯•æ®‹å·®è¿æ¥å’Œå±‚å½’ä¸€åŒ–"""
        utils.print_rank_0("=== æ®‹å·®è¿æ¥è°ƒè¯• ===")

        hidden_states = self.embedding(input_ids.transpose(0, 1))
        attention_mask = torch.ones_like(input_ids, dtype=torch.bool)
        # æ£€æŸ¥Rotary Embedding
        seq_len = hidden_states.size(0)
        position_ids = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0)
        cos, sin = self.rotary_emb(hidden_states, position_ids)
        utils.print_rank_0(f"Rotary cos - å½¢çŠ¶: {cos.shape}, èŒƒå›´: [{cos.min():.3f}, {cos.max():.3f}]")
        utils.print_rank_0(f"Rotary sin - å½¢çŠ¶: {sin.shape}, èŒƒå›´: [{sin.min():.3f}, {sin.max():.3f}]")
        cos_sin = torch.cat([cos, sin], dim=-1).transpose(1, 0).contiguous()

        rotary_pos_emb = cos_sin, cos_sin

        for layer_idx in range(3):
            utils.print_rank_0(f"\n--- ç¬¬{layer_idx}å±‚æ®‹å·®è¿æ¥ ---")
            layer = self.layers[layer_idx]

            # è¾“å…¥ç»Ÿè®¡
            input_mean = hidden_states.mean().item()
            input_std = hidden_states.std().item()

            utils.print_rank_0(f"input_mean: mean={input_mean:.6f}, input_std={input_std:.6f}")

            # è¾“å…¥å±‚å½’ä¸€åŒ–
            norm_input = layer.input_layernorm(hidden_states)
            utils.print_rank_0(f"è¾“å…¥å±‚å½’ä¸€åŒ–: mean={norm_input.mean():.6f}, std={norm_input.std():.6f}")

            # æ³¨æ„åŠ›è¾“å‡º
            attention_output = layer.self_attention(norm_input, attention_mask, rotary_pos_emb)
            if isinstance(attention_output, tuple):
                attention_output = attention_output[0]

            # æ®‹å·®è¿æ¥1
            residual1 = hidden_states + attention_output
            utils.print_rank_0(f"æ®‹å·®è¿æ¥1: mean={residual1.mean():.6f}, std={residual1.std():.6f}")

            # MLPå±‚å½’ä¸€åŒ–
            norm_mlp = layer.pre_mlp_layernorm(residual1)
            utils.print_rank_0(f"MLPå±‚å½’ä¸€åŒ–: mean={norm_mlp.mean():.6f}, std={norm_mlp.std():.6f}")

            # MLPè¾“å‡º
            mlp_output = layer.mlp(norm_mlp)

            # æ®‹å·®è¿æ¥2
            residual2 = residual1 + mlp_output[0]
            utils.print_rank_0(f"æ®‹å·®è¿æ¥2: mean={residual2.mean():.6f}, std={residual2.std():.6f}")

            hidden_states = residual2

    def debug_lm_head_output(self, input_ids):
        """è°ƒè¯•LM Headè¾“å‡º"""
        utils.print_rank_0("=== LM Headè¾“å‡ºè°ƒè¯• ===")

        # å®Œæ•´å‰å‘ä¼ æ’­åˆ°æœ€åä¸€å±‚
        hidden_states = self.embedding(input_ids.transpose(0, 1))
        attention_mask = torch.ones_like(input_ids, dtype=torch.bool)
        # æ£€æŸ¥Rotary Embedding
        seq_len = hidden_states.size(0)
        position_ids = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0)
        cos, sin = self.rotary_emb(hidden_states, position_ids)
        utils.print_rank_0(f"Rotary cos - å½¢çŠ¶: {cos.shape}, èŒƒå›´: [{cos.min():.3f}, {cos.max():.3f}]")
        utils.print_rank_0(f"Rotary sin - å½¢çŠ¶: {sin.shape}, èŒƒå›´: [{sin.min():.3f}, {sin.max():.3f}]")
        cos_sin = torch.cat([cos, sin], dim=-1).transpose(1, 0).contiguous()

        rotary_pos_emb = cos_sin, cos_sin

        for layer in self.layers:
            layer_output = layer(hidden_states, attention_mask, rotary_pos_emb)
            if isinstance(layer_output, tuple):
                hidden_states = layer_output[0]
            else:
                hidden_states = layer_output

        # æœ€ç»ˆå½’ä¸€åŒ–
        if self.final_norm:
            hidden_states = self.final_norm(hidden_states)
            utils.print_rank_0(f"æœ€ç»ˆå½’ä¸€åŒ–è¾“å‡º: mean={hidden_states.mean():.6f}, std={hidden_states.std():.6f}")

        # LM Headå‰å‘ä¼ æ’­
        lm_output = self.lm_head(hidden_states)
        utils.print_rank_0(f"LM Headè¾“å‡ºç±»å‹: {type(lm_output)}")

        if isinstance(lm_output, tuple):
            utils.print_rank_0(f"LM Head tupleé•¿åº¦: {len(lm_output)}")
            for i, _ in enumerate(lm_output):
                output = lm_output[i]
                if output is not None:
                    utils.print_rank_0(
                        f"  è¾“å‡º{i}: å½¢çŠ¶={output.shape}, mean={output.mean():.6f}, std={output.std():.6f}")
            # é€šå¸¸ç¬¬ä¸€ä¸ªå…ƒç´ æ˜¯logits
            logits = lm_output[0]
        else:
            logits = lm_output

        logits = logits.transpose(0, 1)
        utils.print_rank_0(f"æœ€ç»ˆlogits: å½¢çŠ¶={logits.shape}, mean={logits.mean():.6f}, std={logits.std():.6f}")

        return logits

    def debug_mlp_implementation(self, input_ids):
        """è¯¦ç»†è°ƒè¯•MLPå®ç°"""
        utils.print_rank_0("=== MLPå®ç°è¯¦ç»†è°ƒè¯• ===")

        hidden_states = self.embedding(input_ids.transpose(0, 1))
        attention_mask = torch.ones_like(input_ids, dtype=torch.bool)
        # æ£€æŸ¥Rotary Embedding
        seq_len = hidden_states.size(0)
        position_ids = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0)
        cos, sin = self.rotary_emb(hidden_states, position_ids)
        utils.print_rank_0(f"Rotary cos - å½¢çŠ¶: {cos.shape}, èŒƒå›´: [{cos.min():.3f}, {cos.max():.3f}]")
        utils.print_rank_0(f"Rotary sin - å½¢çŠ¶: {sin.shape}, èŒƒå›´: [{sin.min():.3f}, {sin.max():.3f}]")
        cos_sin = torch.cat([cos, sin], dim=-1).transpose(1, 0).contiguous()
        rotary_pos_emb = cos_sin, cos_sin

        # åªè¿è¡Œåˆ°ç¬¬2å±‚
        for layer_idx in range(3):
            layer = self.layers[layer_idx]

            # è¿è¡Œåˆ°MLPè¾“å…¥
            attention_output = layer.self_attention(hidden_states, attention_mask, rotary_pos_emb)
            if isinstance(attention_output, tuple):
                attention_output = attention_output[0]
            residual1 = hidden_states + attention_output
            mlp_input = layer.pre_mlp_layernorm(residual1)

            if layer_idx <= 2:  # é‡ç‚¹è°ƒè¯•ç¬¬2å±‚
                utils.print_rank_0(f"\n--- ç¬¬{layer_idx}å±‚MLPè¯¦ç»†è°ƒè¯• ---")
                utils.print_rank_0(f"MLPè¾“å…¥: mean={mlp_input.mean():.6f}, std={mlp_input.std():.6f}")

                mlp = layer.mlp
                # æ£€æŸ¥MLPå„ç»„ä»¶
                utils.print_rank_0(f"MLPç±»å‹: {type(mlp)}")
                utils.print_rank_0(f"MLPé…ç½®: gated_linear_unit={mlp.config.gated_linear_unit}")

                # é€æ­¥æ‰§è¡ŒMLPå‰å‘ä¼ æ’­
                # 1. linear_fc1
                fc1_output = mlp.linear_fc1(mlp_input)[0]
                utils.print_rank_0(
                    f"linear_fc1è¾“å‡º: shape={fc1_output.shape}, mean={fc1_output.mean():.6f}, std={fc1_output.std():.6f}")

                # 2. æ£€æŸ¥æ˜¯å¦åˆ†ä¸ºgateå’Œup
                if mlp.config.gated_linear_unit:
                    gate, up = torch.chunk(fc1_output, 2, dim=-1)
                    utils.print_rank_0(f"gateéƒ¨åˆ†: shape={gate.shape}, mean={gate.mean():.6f}, std={gate.std():.6f}")
                    utils.print_rank_0(f"upéƒ¨åˆ†: shape={up.shape}, mean={up.mean():.6f}, std={up.std():.6f}")

                    # 3. æ¿€æ´»å‡½æ•°
                    activated_gate = torch.nn.functional.silu(gate)
                    utils.print_rank_0(f"SiLU(gate): mean={activated_gate.mean():.6f}, std={activated_gate.std():.6f}")

                    # 4. é—¨æ§ä¹˜æ³•
                    intermediate = activated_gate * up
                    utils.print_rank_0(f"gate * up: mean={intermediate.mean():.6f}, std={intermediate.std():.6f}")
                else:
                    intermediate = mlp.activation_func(fc1_output)
                    utils.print_rank_0(f"æ¿€æ´»å‡½æ•°è¾“å‡º: mean={intermediate.mean():.6f}, std={intermediate.std():.6f}")

                # 5. linear_fc2
                fc2_output = mlp.linear_fc2(intermediate)[0]
                utils.print_rank_0(f"linear_fc2è¾“å‡º: mean={fc2_output.mean():.6f}, std={fc2_output.std():.6f}")

                # 6. dropout (å¦‚æœæœ‰)
                if hasattr(mlp, 'dropout') and mlp.dropout.p > 0:
                    final_output = mlp.dropout(fc2_output)
                    utils.print_rank_0(f"Dropoutå: mean={final_output.mean():.6f}, std={final_output.std():.6f}")
                else:
                    final_output = fc2_output

                utils.print_rank_0(f"MLPæœ€ç»ˆè¾“å‡º: mean={final_output.mean():.6f}, std={final_output.std():.6f}")

            # å®Œæˆè¿™ä¸€å±‚
            layer_output = layer(hidden_states)
            if isinstance(layer_output, tuple):
                hidden_states = layer_output[0]
            else:
                hidden_states = layer_output

    def check_mlp_weights(self, layer_idx=2):
        """æ£€æŸ¥MLPå±‚æƒé‡"""
        utils.print_rank_0(f"=== ç¬¬{layer_idx}å±‚MLPæƒé‡æ£€æŸ¥ ===")

        layer = self.layers[layer_idx]
        mlp = layer.mlp

        # æ£€æŸ¥linear_fc1æƒé‡
        fc1_weight = mlp.linear_fc1.weight
        fc1_bias = mlp.linear_fc1.bias if hasattr(mlp.linear_fc1, 'bias') else None

        utils.print_rank_0(f"linear_fc1æƒé‡: shape={fc1_weight.shape}")
        utils.print_rank_0(f"linear_fc1æƒé‡ - mean: {fc1_weight.mean().item():.6f}, std: {fc1_weight.std().item():.6f}")
        utils.print_rank_0(f"linear_fc1æƒé‡ - èŒƒå›´: [{fc1_weight.min().item():.6f}, {fc1_weight.max().item():.6f}]")

        if fc1_bias is not None:
            utils.print_rank_0(f"linear_fc1åç½® - mean: {fc1_bias.mean().item():.6f}, std: {fc1_bias.std().item():.6f}")

        # æ£€æŸ¥linear_fc2æƒé‡
        fc2_weight = mlp.linear_fc2.weight
        fc2_bias = mlp.linear_fc2.bias if hasattr(mlp.linear_fc2, 'bias') else None

        utils.print_rank_0(f"linear_fc2æƒé‡: shape={fc2_weight.shape}")
        utils.print_rank_0(f"linear_fc2æƒé‡ - mean: {fc2_weight.mean().item():.6f}, std: {fc2_weight.std().item():.6f}")
        utils.print_rank_0(f"linear_fc2æƒé‡ - èŒƒå›´: [{fc2_weight.min().item():.6f}, {fc2_weight.max().item():.6f}]")

        if fc2_bias is not None:
            utils.print_rank_0(f"linear_fc2åç½® - mean: {fc2_bias.mean().item():.6f}, std: {fc2_bias.std().item():.6f}")


class Qwen2MegatronCritic(Qwen2MegatronModel):
    """PPO Criticæ¨¡å‹ï¼ˆä»·å€¼ç½‘ç»œï¼‰ï¼Œå…¼å®¹Megatron TPå¹¶è¡Œ"""

    def __init__(
            self,
            g_config,
            qwen_config: Qwen2Config,
            megatron_config: TransformerConfig,
            freeze_actor_backbone: bool = True,  # æ˜¯å¦å†»ç»“Actoråº•å±‚å‚æ•°
            use_bias: bool = True  # ä»·å€¼å¤´æ˜¯å¦ä½¿ç”¨åç½®
    ):
        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–ï¼ˆå¤ç”¨åµŒå…¥å±‚ã€Transformerå±‚ã€LayerNormç­‰ï¼‰
        super().__init__(g_config=g_config, qwen_config=qwen_config, megatron_config=megatron_config)
        self.micro_batch_size = g_config.critic.ppo_micro_batch_size
        self.freeze_actor_backbone = freeze_actor_backbone

        # 2. ä»·å€¼è¾“å‡ºå¤´ï¼ˆå…¼å®¹TPå¹¶è¡Œï¼‰
        # è¾“å…¥ï¼šActorçš„hidden_sizeï¼ˆå·²æŒ‰TPæ‹†åˆ†ï¼Œæ¯ä¸ªè¿›ç¨‹ä»…æŒæœ‰ hidden_size/TP_size ç»´åº¦ï¼‰
        # è¾“å‡ºï¼šæ ‡é‡ä»·å€¼ï¼ˆç»´åº¦=1ï¼Œæ— éœ€TPæ‹†åˆ†ï¼‰

        # è¾“å‡ºå±‚ï¼ˆfinal_norm + lm_headï¼‰ï¼šä»… PP æœ€åä¸€ä¸ª stage åˆå§‹åŒ–
        self.value_head = None
        if self.pp_rank == self.pp_size - 1:
            self.lm_head = None
            self.value_head = nn.Linear(
                in_features=self.hidden_size,
                out_features=1,  # è¾“å‡ºæ ‡é‡ä»·å€¼
                bias=False,
            )

            # åˆå§‹åŒ–åˆ†ç±»å¤´ï¼ˆå¯é€‰ï¼šè‹¥ä»é¢„è®­ç»ƒæ¨¡å‹åŠ è½½ï¼Œå¯è·³è¿‡ï¼›è‹¥éšæœºåˆå§‹åŒ–ï¼Œå»ºè®®ç”¨Xavierï¼‰
            if parallel_state.get_tensor_model_parallel_rank() == 0:
                nn.init.xavier_uniform_(self.value_head.weight)
                if hasattr(self.value_head, 'bias') and self.value_head.bias is not None:
                    nn.init.zeros_(self.value_head.bias)
            else:
                nn.init.zeros_(self.value_head.weight)
                if hasattr(self.value_head, 'bias') and self.value_head.bias is not None:
                    nn.init.zeros_(self.value_head.bias)

            self.value_head.weight.data = self.value_head.weight.data.to('cuda')
            # å¹¿æ’­rank0çš„å‚æ•°åˆ°æ‰€æœ‰TP rank
            torch.distributed.broadcast(
                self.value_head.weight.data,
                src=0,  # TP groupå†…çš„rank0
                group=parallel_state.get_tensor_model_parallel_group()
            )

        # 4. å†»ç»“Actoråº•å±‚å‚æ•°ï¼ˆå¯é€‰ï¼Œæ ¹æ®è®­ç»ƒç­–ç•¥è°ƒæ•´ï¼‰
        if self.freeze_actor_backbone:
            self._freeze_actor_components()

    def _freeze_actor_components(self):
        """å†»ç»“Actorçš„åº•å±‚ç‰¹å¾æå–ç»„ä»¶ï¼Œä»…è®­ç»ƒä»·å€¼å¤´"""
        if not self.embedding:
            for param in self.embedding.parameters():
                param.requires_grad = False
        if not self.rotary_emb:
            for param in self.rotary_emb.parameters():
                param.requires_grad = False
        for param in self.layers.parameters():
            param.requires_grad = False
        if not self.final_norm:
            self.final_norm.requires_grad_(False)  # RMSNormé€šå¸¸ä¹Ÿå†»ç»“

    def forward(
            self,
            input_ids: torch.Tensor,
            attention_mask: torch.Tensor = None,
            only_last_token: bool = True,  # PPOé»˜è®¤ä»…éœ€æœ€åä¸€ä¸ªtokençš„ä»·å€¼
            normalize_value: bool = False  # æ˜¯å¦å¯¹ä»·å€¼é¢„æµ‹å½’ä¸€åŒ–ï¼ˆç¨³å®šè®­ç»ƒï¼‰
    ) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­ï¼šè¾“å‡ºåºåˆ—çš„ä»·å€¼é¢„æµ‹
        Args:
            input_ids: [batch_size, seq_len] è¾“å…¥token ID
            attention_mask: [batch_size, seq_len] æ³¨æ„åŠ›æ©ç ï¼ˆå¤„ç†paddingï¼‰
            only_last_token: æ˜¯å¦ä»…è¿”å›æœ€åä¸€ä¸ªtokençš„ä»·å€¼ï¼ˆä»£è¡¨æ•´ä¸ªåºåˆ—çš„çŠ¶æ€ä»·å€¼ï¼‰
            normalize_value: æ˜¯å¦å¯¹ä»·å€¼é¢„æµ‹åšLayerNormï¼ˆç¨³å®šPPOè®­ç»ƒï¼‰
        Returns:
            value_preds: ä»·å€¼é¢„æµ‹ï¼Œç»´åº¦ä¸ºï¼š
                - only_last_token=True: [batch_size, 1]ï¼ˆPPOæ ‡å‡†è¾“å‡ºï¼‰
                - only_last_token=False: [batch_size, seq_len, 1]ï¼ˆæ—¶åºä»·å€¼é¢„æµ‹ï¼‰
        """
        # -------------------------- 1. ç¬¬ä¸€ä¸ª PP stageï¼šæ‰§è¡ŒåµŒå…¥å±‚ + Rotary ç¼–ç  --------------------------
        # åµŒå…¥ + Rotary ç¼–ç 
        rotary_pos_emb = None
        if self.pp_rank == 0:
            ori_input_ids = input_ids
            # [b, s, h] -> [s, b, h]
            input_ids = input_ids.transpose(1, 0).contiguous()
            # 1. åµŒå…¥å±‚ + Rotaryç¼–ç ï¼ˆä¸Actorå®Œå…¨ä¸€è‡´ï¼‰
            hidden_states = self.embedding(input_ids)  # [batch, seq_len, hidden_size/TP_size]

            ori_input_ids.transpose(1, 0)
        else:
            # self.hidden_states should be passed by Megatron
            hidden_states = self.input_tensor

        # -------------------------- ä¿®æ­£æ³¨æ„åŠ›æ©ç ç»´åº¦ --------------------------
        if attention_mask is not None:
            # è‡ªæ³¨æ„åŠ›éœ€è¦çš„æ©ç å½¢çŠ¶ï¼š[batch_size, 1, seq_len, seq_len]
            # 1. å°† [batch_size, seq_len] æ‰©å±•ä¸º [batch_size, 1, 1, seq_len]
            attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)

        seq_len = hidden_states.size(0)
        position_ids = torch.arange(0, seq_len, device=hidden_states.device).unsqueeze(0)
        # è®¡ç®— Rotary åµŒå…¥ï¼ˆä»… stage 0 è®¡ç®—ï¼Œä¼ é€’ç»™åç»­ stageï¼‰
        cos, sin = self.rotary_emb(hidden_states, position_ids)  # [b, s, h]
        cos_sin = torch.cat([cos, sin], dim=-1).transpose(1, 0).contiguous()

        rotary_pos_emb = cos_sin, cos_sin

        # -------------------------- 2. å½“å‰ stage å¤„ç†è‡ªå·±çš„ Transformer å±‚ --------------------------
        # 2. Transformerå±‚ä¼ æ’­ï¼ˆå®Œæ•´åºåˆ—ï¼Œä¸æå‰æˆªæ–­ï¼Œä¿è¯ç‰¹å¾å®Œæ•´æ€§ï¼‰
        for layer in self.layers:
            hidden_states = layer(
                hidden_states, attention_mask=attention_mask, rotary_pos_emb=rotary_pos_emb
            )
            # å–å…ƒç»„çš„ç¬¬ä¸€ä¸ªå…ƒç´ ä½œä¸ºæ–°çš„ hidden_statesï¼ˆå¿½ç•¥ context ä¿¡æ¯ï¼‰
            hidden_states = hidden_states[0] if isinstance(hidden_states, tuple) else hidden_states

        # è¾“å‡º
        if self.pp_rank == self.pp_size - 1:
            # 3. æœ€ç»ˆå½’ä¸€åŒ–
            hidden_states = self.final_norm(hidden_states)

            # 4. ä»·å€¼é¢„æµ‹ï¼ˆæ ‡é‡è¾“å‡ºï¼‰
            value_preds = self.value_head(hidden_states)  # [batch, seq_len, 1]

            # [s, b, h] -> [b, s, h]
            value_preds = value_preds.transpose(1, 0).contiguous()

            # 5. ä»…ä¿ç•™æœ€åä¸€ä¸ªtokençš„ä»·å€¼ï¼ˆPPOæ ¸å¿ƒç”¨æ³•ï¼‰
            if only_last_token:
                value_preds = value_preds[:, -1:, :].squeeze(-1)  # [batch, 1]

            # 6. ä»·å€¼å½’ä¸€åŒ–ï¼ˆå¯é€‰ï¼Œç¼“è§£PPOè®­ç»ƒä¸ç¨³å®šæ€§ï¼‰
            if normalize_value:
                value_preds = nn.functional.layer_norm(
                    value_preds, normalized_shape=value_preds.shape[1:], eps=1e-5
                )
            value_preds = value_preds.float()
            value_preds = torch.squeeze(value_preds, dim=-1)
            return value_preds

        return hidden_states

    def forward_backward_batch(self, batch: TensorDict, only_last_token=False,
                               forward_only=False, post_process_fn=None, meta_info: Dict = None):
        # broadcast from last pp rank to all other pp ranks
        batch = batch.contiguous()
        torch_functional.broadcast_dict_tensor(batch,
                                               src=parallel_state.get_pipeline_model_parallel_last_rank(),
                                               group=parallel_state.get_pipeline_model_parallel_group())
        # split into micro-batches
        batch['attention_mask'] = batch['attention_mask'].to(bool)
        batches = self.split_dict_tensor_into_batches(batch, batch_size=self.micro_batch_size)
        n_micro_batch = len(batches)
        seq_len = batches[0]['input_ids'].shape[1]

        forward_backward_func = pipeline_parallel.get_forward_backward_func()

        def loss_func(output, data):
            if forward_only:
                return 1.0, output

            responses = data['responses']
            attention_mask = data['attention_mask']
            values = data['values']
            returns = data['returns']
            response_length = responses.size(1)

            eos_mask = attention_mask[:, -response_length:]

            cliprange_value = meta_info['cliprange_value']

            vpreds = output  # (bs, sequence_length)
            vpreds = vpreds[:, -response_length - 1:-1]

            vf_loss, vf_clipfrac = core_algos.compute_value_loss(vpreds=vpreds,
                                                                 values=values,
                                                                 returns=returns,
                                                                 eos_mask=eos_mask,
                                                                 cliprange_value=cliprange_value)
            stats = {
                'critic/vf_loss': vf_loss.detach().item(),
                'critic/vf_clipfrac': vf_clipfrac.detach().item(),
                'critic/vpred_mean': utils.masked_mean(vpreds, eos_mask).detach().item(),
            }

            return vf_loss, stats

        def forward_step(batch_iter, model):
            micro_batch = next(batch_iter)
            output = model(input_ids=micro_batch["input_ids"], attention_mask=None,
                           only_last_token=only_last_token)
            return output, partial(loss_func, data=batch)

        # batch should be a list of batches inside micro-batches
        batch_generator = self.make_batch_generator(batches, vpp_size=1)

        # for flash-attn: (seq_len, batch_size, hidden_size) = (mbs*seq_len, 1, hidden_size)
        if parallel_state.get_pipeline_model_parallel_world_size() > 1:
            losses_reduced = forward_backward_func(
                forward_step_func=forward_step,
                data_iterator=batch_generator,
                model=self,
                num_microbatches=n_micro_batch,
                seq_length=seq_len,
                # hidden_size=self.model_config.hidden_size,
                micro_batch_size=self.micro_batch_size,
                forward_only=forward_only,
            )
        else:
            losses_reduced = forward_backward_func(
                forward_step_func=forward_step,
                data_iterator=batch_generator,
                model=self,
                num_microbatches=n_micro_batch,
                seq_length=seq_len,
                # hidden_size=self.model_config.hidden_size,
                micro_batch_size=self.micro_batch_size,
                forward_only=forward_only,
            )
        # loss_reduces contains the stats returned from loss_func
        return losses_reduced


def build_qwen2_megatron_model(config, tokenizer, qwen_model_path: str, lora_config: LoraConfig = None, is_critic=False) \
        -> Union[Qwen2MegatronModel, Qwen2MegatronCritic]:
    """æ„å»º Megatron å¹¶è¡ŒåŒ– Qwen2.5 æ¨¡å‹ï¼Œå¯é›†æˆ Lora"""
    qwen_config = Qwen2Config.from_pretrained(qwen_model_path)

    utils.print_rank_0(f"qwen2 config: {qwen_config}")

    params_dtype = get_params_dtype(config)

    # é…ç½® Megatron å¹¶è¡Œå‚æ•°ï¼ˆéœ€ä¸ DeepSpeed å¯¹é½ï¼‰
    megatron_config = TransformerConfig(
        hidden_size=qwen_config.hidden_size,  # 2048
        num_layers=qwen_config.num_hidden_layers,
        num_attention_heads=qwen_config.num_attention_heads,  # 16
        num_query_groups=qwen_config.num_key_value_heads,  # 2
        kv_channels=qwen_config.hidden_size // qwen_config.num_attention_heads,  # 128
        ffn_hidden_size=qwen_config.intermediate_size,
        layernorm_epsilon=qwen_config.rms_norm_eps,
        init_method=torch.nn.init.xavier_uniform_,
        output_layer_init_method=torch.nn.init.xavier_uniform_,
        # activation_func_clamp_value=10.0,
        tensor_model_parallel_size=parallel_state.get_tensor_model_parallel_world_size(),
        pipeline_model_parallel_size=parallel_state.get_pipeline_model_parallel_world_size(),
        params_dtype=params_dtype,
        pipeline_dtype=params_dtype,
        bf16=config.deepspeed.bf16.enabled,
        fp16=config.deepspeed.fp16.enabled,
        activation_func=ACT2FN[qwen_config.hidden_act],
        gated_linear_unit=True,
        add_bias_linear=False,
        add_qkv_bias=True,
        attention_dropout=qwen_config.attention_dropout,
        hidden_dropout=0.0,
        attention_softmax_in_fp32=True,
    )
    utils.print_rank_0(f"megatron TransformerConfig: {megatron_config}")
    # åŠ è½½é¢„è®­ç»ƒæƒé‡ï¼ˆHugging Face -> Megatron æ ¼å¼æ˜ å°„ï¼‰
    # æ³¨ï¼šéœ€æ‰‹åŠ¨æ˜ å°„å‚æ•°åï¼ˆå¦‚ 'embed_tokens.weight' -> 'embedding.weight'ï¼‰
    hf_model = Qwen2ForCausalLM.from_pretrained(qwen_model_path, dtype=params_dtype).cuda()
    utils.print_rank_0(hf_model)
    utils.print_rank_0(f"hf_model qwen2 config: {hf_model.config}")

    # response = tokenizer.decode(hf_model.generate(inputs=tokenizer.encode("Hello World!", return_tensors="pt").to(hf_model.device),
    #                                               max_length=512,
    #                                               max_new_tokens=100,
    #                                               eos_token_id=151643,
    #                                               use_cache=True,
    #                                               pad_token_id=0,)[0],  # è‹¥batch_size>1ï¼Œéœ€éå†æ‰€æœ‰æ ·æœ¬
    #                         skip_special_tokens=True)
    # utils.print_rank_0(f"qwen2 response: {response}")

    if not is_critic:
        model = Qwen2MegatronModel(config, qwen_config, megatron_config)
        model.cuda()
        qwen_load.load_state_dict_to_megatron_qwen(hf_model.state_dict(), [model], qwen_config,
                                                   megatron_config.params_dtype)
    else:
        model = Qwen2MegatronCritic(config, hf_model.config, megatron_config)
        model.cuda()
        qwen_load.load_state_dict_to_megatron_qwen(hf_model.state_dict(), [model], hf_model.config,
                                                   megatron_config.params_dtype, is_value_model=is_critic)

    # é›†æˆLoRaï¼ˆä»…å¯¹ Attention å±‚çš„ proj å±‚æ·»åŠ  LoRaï¼‰
    # if lora_config is not None:
    #     target_modules = ["query_proj", "key_proj", "value_proj", "output_proj"]
    #     lora_config.target_modules = target_modules
    #     model = get_peft_model(model, lora_config)
    #     model.print_trainable_parameters() # éªŒè¯ LoRa è®­ç»ƒå‚æ•°å æ¯”ï¼ˆé€šå¸¸ <1%ï¼‰

    diffs = utils.find_tensor_diff(hf_model.model.embed_tokens.weight, model.embedding.weight)
    utils.print_rank_0(f"model.embedding.weightå·®å¼‚ä½ç½®ï¼š{diffs}")
    for i in range(len(hf_model.model.layers)):
        diffs = utils.find_tensor_diff(hf_model.model.layers[i].input_layernorm.weight,
                                       model.layers[i].input_layernorm.weight)
        utils.print_rank_0(f"model.layers[{i}].input_layernorm.weightï¼š{diffs}")
        hf_qkv = torch.cat(
            [hf_model.model.layers[i].self_attn.q_proj.weight, hf_model.model.layers[i].self_attn.k_proj.weight,
             hf_model.model.layers[i].self_attn.v_proj.weight],
            dim=0)
        diffs = utils.find_tensor_diff(hf_qkv, model.layers[i].self_attention.linear_qkv.weight)
        utils.print_rank_0(f"model.layers[{i}].self_attention.linear_qkv.weightå·®å¼‚ä½ç½®ï¼š{diffs}")
        hf_qkv_bias = torch.cat(
            [hf_model.model.layers[i].self_attn.q_proj.bias, hf_model.model.layers[i].self_attn.k_proj.bias,
             hf_model.model.layers[i].self_attn.v_proj.bias],
            dim=0)
        diffs = utils.find_tensor_diff(hf_qkv_bias, model.layers[i].self_attention.linear_qkv.bias)
        utils.print_rank_0(f"model.layers[{i}].self_attention.linear_qkv.biaså·®å¼‚ä½ç½®ï¼š{diffs}")
        diffs = utils.find_tensor_diff(hf_model.model.layers[i].self_attn.o_proj.weight,
                                       model.layers[i].self_attention.linear_proj.weight)
        utils.print_rank_0(f"model.layers[{i}].self_attention.linear_proj.biasï¼š{diffs}")
        linear_fc1 = torch.cat(
            [hf_model.model.layers[i].mlp.gate_proj.weight, hf_model.model.layers[i].mlp.up_proj.weight],
            dim=0)
        diffs = utils.find_tensor_diff(linear_fc1, model.layers[i].mlp.linear_fc1.weight)
        utils.print_rank_0(f"model.layers[{i}].mlp.linear_fc1.weightï¼š{diffs}")
        diffs = utils.find_tensor_diff(hf_model.model.layers[i].mlp.down_proj.weight,
                                       model.layers[i].mlp.linear_fc2.weight)
        utils.print_rank_0(f"model.layers[{i}].mlp.linear_fc2.weightï¼š{diffs}")
        diffs = utils.find_tensor_diff(hf_model.model.layers[i].post_attention_layernorm.weight,
                                       model.layers[i].pre_mlp_layernorm.weight)
        utils.print_rank_0(f"model.layers[{i}].pre_mlp_layernorm.weightï¼š{diffs}")

    diffs = utils.find_tensor_diff(hf_model.model.norm.weight, model.final_norm.weight)
    utils.print_rank_0(f"model.final_norm.weightå·®å¼‚ä½ç½®ï¼š{diffs}")
    if not is_critic:
        diffs = utils.find_tensor_diff(hf_model.lm_head.weight, model.lm_head.weight)
        utils.print_rank_0(f"model.lm_head.weightå·®å¼‚ä½ç½®ï¼š{diffs}")

    run_comprehensive_debug(hf_model, tokenizer)
    return model


def get_params_dtype(config):
    params_dtype = torch.get_default_dtype()
    if config.deepspeed.bf16.enabled:
        params_dtype = torch.bfloat16
    if config.deepspeed.fp16.enabled:
        params_dtype = torch.float16
    return params_dtype


def detailed_forward_debug(self, input_ids, attention_mask=None):
    """è¯¦ç»†çš„å‰å‘ä¼ æ’­è°ƒè¯•"""
    utils.print_rank_0("=== hf_model è¯¦ç»†å‰å‘ä¼ æ’­è°ƒè¯•å¼€å§‹ ===")

    # åŸå§‹è¾“å…¥
    utils.print_rank_0(f"è¾“å…¥å½¢çŠ¶: {input_ids.shape}")
    utils.print_rank_0(f"è¾“å…¥token: {input_ids[0].cpu().numpy()}")

    # åµŒå…¥å±‚
    hidden_states = self.model.embed_tokens(input_ids)
    utils.print_rank_0(
        f"åµŒå…¥å±‚è¾“å‡º - å½¢çŠ¶: {hidden_states.shape}, å‡å€¼: {hidden_states.mean():.6f}, æ ‡å‡†å·®: {hidden_states.std():.6f}")

    # æ£€æŸ¥Rotary Embedding
    use_cache: Optional[bool] = None
    cache_position: Optional[torch.LongTensor] = None
    past_key_values: Optional[Cache] = None
    position_ids: Optional[torch.LongTensor] = None
    if use_cache and past_key_values is None:
        past_key_values = DynamicCache(config=self.model.config)

    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position = torch.arange(
            past_seen_tokens, past_seen_tokens + hidden_states.shape[1], device=hidden_states.device
        )

    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)

    rotary_pos_emb = self.model.rotary_emb(hidden_states, position_ids)
    cos, sin = rotary_pos_emb
    utils.print_rank_0(f"Rotary cos - å½¢çŠ¶: {cos.shape}, èŒƒå›´: [{cos.min():.3f}, {cos.max():.3f}]")
    utils.print_rank_0(f"Rotary sin - å½¢çŠ¶: {sin.shape}, èŒƒå›´: [{sin.min():.3f}, {sin.max():.3f}]")

    # It may already have been prepared by e.g. `generate`
    if not isinstance(causal_mask_mapping := attention_mask, dict):
        # Prepare mask arguments
        mask_kwargs = {
            "config": self.model.config,
            "input_embeds": hidden_states,
            "attention_mask": attention_mask,
            "cache_position": cache_position,
            "past_key_values": past_key_values,
            "position_ids": position_ids,
        }
        # Create the masks
        causal_mask_mapping = {
            "full_attention": create_causal_mask(**mask_kwargs),
        }

    # é€å±‚æ£€æŸ¥Transformer
    for layer_idx, layer in enumerate(self.model.layers):
        utils.print_rank_0(f"\n--- ç¬¬{layer_idx}å±‚ ---")

        # è¾“å…¥ç»Ÿè®¡
        input_mean = hidden_states.mean().item()
        input_std = hidden_states.std().item()
        utils.print_rank_0(f"è¾“å…¥ - å‡å€¼: {input_mean:.6f}, æ ‡å‡†å·®: {input_std:.6f}")

        # å±‚å‰å‘ä¼ æ’­
        layer_output = layer(
            hidden_states,
            attention_mask=causal_mask_mapping[layer.attention_type],
            position_ids=position_ids,
            past_key_values=past_key_values,
            use_cache=use_cache,
            cache_position=cache_position,
            position_embeddings=rotary_pos_emb,
            # **kwargs,
        )

        # å¤„ç†è¾“å‡ºï¼ˆå¯èƒ½æ˜¯tupleï¼‰
        if isinstance(layer_output, tuple):
            utils.print_rank_0(f"å±‚è¾“å‡ºæ˜¯tupleï¼Œé•¿åº¦: {len(layer_output)}")
            hidden_states = layer_output[0]
            if len(layer_output) > 1:
                utils.print_rank_0(
                    f"é¢å¤–è¾“å‡º: {[x.shape if hasattr(x, 'shape') else type(x) for x in layer_output[1:]]}")
        else:
            hidden_states = layer_output

        # è¾“å‡ºç»Ÿè®¡
        output_mean = hidden_states.mean().item()
        output_std = hidden_states.std().item()
        utils.print_rank_0(f"è¾“å‡º - å‡å€¼: {output_mean:.6f}, æ ‡å‡†å·®: {output_std:.6f}")

        # æ£€æŸ¥å˜åŒ–
        change = abs(output_mean - input_mean)
        utils.print_rank_0(f"å‡å€¼å˜åŒ–: {change:.6f}")

        # æ£€æŸ¥NaN/Inf
        if torch.isnan(hidden_states).any():
            utils.print_rank_0(f"âš ï¸  ç¬¬{layer_idx}å±‚è¾“å‡ºåŒ…å«NaN!")
        if torch.isinf(hidden_states).any():
            utils.print_rank_0(f"âš ï¸  ç¬¬{layer_idx}å±‚è¾“å‡ºåŒ…å«Inf!")

        # åªæ£€æŸ¥å‰3å±‚ï¼Œé¿å…è¾“å‡ºè¿‡å¤š
        if layer_idx >= 2:
            utils.print_rank_0("... (è·³è¿‡åç»­å±‚è¯¦ç»†è¾“å‡º)")
            break

    # æœ€ç»ˆå±‚
    if self.model.norm:
        hidden_states = self.model.norm(hidden_states)
        utils.print_rank_0(f"æœ€ç»ˆå½’ä¸€åŒ– - å‡å€¼: {hidden_states.mean():.6f}, æ ‡å‡†å·®: {hidden_states.std():.6f}")

    # LM Head
    hidden_states = self.lm_head(hidden_states)
    if isinstance(hidden_states, tuple):
        utils.print_rank_0(f"lm_headè¾“å‡ºæ˜¯tupleï¼Œé•¿åº¦: {len(hidden_states)}")
        hidden_states = hidden_states[0]
        if len(hidden_states) > 1:
            utils.print_rank_0(
                f"lm_headé¢å¤–è¾“å‡º: {[x.shape if hasattr(x, 'shape') else type(x) for x in hidden_states[1:]]}")
    else:
        hidden_states = hidden_states
    logits = hidden_states
    utils.print_rank_0(f"æœ€ç»ˆlogits - å½¢çŠ¶: {logits.shape}, å‡å€¼: {logits.mean():.6f}, æ ‡å‡†å·®: {logits.std():.6f}")

    # æ£€æŸ¥logitsçš„åˆç†æ€§
    top5_values, top5_indices = torch.topk(logits[0, -1], 5)
    utils.print_rank_0(f"æœ€åä¸€ä¸ªtokençš„top-5 logits:")
    for i, (value, idx) in enumerate(zip(top5_values, top5_indices)):
        utils.print_rank_0(f"  {i + 1}. token {idx.item()}: {value.item():.3f}")

    return logits


def run_comprehensive_debug(self, tokenizer):
    """è¿è¡Œå…¨é¢çš„è°ƒè¯•"""
    utils.print_rank_0("ğŸš€ hf_model å¼€å§‹å…¨é¢è°ƒè¯•...")

    # ä½¿ç”¨å›ºå®šçš„ç®€å•è¾“å…¥
    test_prompt = "Hello"
    if tokenizer is not None:
        input_ids = tokenizer.encode(test_prompt, return_tensors="pt").to('cuda')
    else:
        # å¦‚æœæ²¡æœ‰tokenizerï¼Œä½¿ç”¨ç®€å•æ•°å­—
        input_ids = torch.tensor([[1, 2, 3]], device='cuda')

    utils.print_rank_0(f"æµ‹è¯•è¾“å…¥: '{test_prompt}' -> {input_ids.cpu().numpy()}")

    # 1. è¯¦ç»†å‰å‘ä¼ æ’­
    utils.print_rank_0("\n" + "=" * 50)
    utils.print_rank_0("1. è¯¦ç»†å‰å‘ä¼ æ’­æ£€æŸ¥")
    utils.print_rank_0("=" * 50)
    logits = detailed_forward_debug(self, input_ids)

    # # 2. ç”Ÿæˆè¿‡ç¨‹é‡‡æ ·æ£€æŸ¥
    # utils.print_rank_0("\n" + "=" * 50)
    # utils.print_rank_0("2. ç”Ÿæˆè¿‡ç¨‹é‡‡æ ·æ£€æŸ¥")
    # utils.print_rank_0("=" * 50)
    # generated = self.debug_generation_sampling(input_ids)

    # 3. éªŒè¯æœ€ç»ˆè¾“å‡º
    # if tokenizer is not None:
    #     generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
    #     utils.print_rank_0(f"\næœ€ç»ˆç”Ÿæˆæ–‡æœ¬: '{generated_text}'")
    # else:
    #     utils.print_rank_0(f"\næœ€ç»ˆç”Ÿæˆtoken: {generated[0].cpu().numpy()}")

    # # è¿è¡Œè¿™äº›è°ƒè¯•å‡½æ•°æ¥å®šä½å…·ä½“é—®é¢˜
    debug_attention_mechanism(self, input_ids)
    # self.debug_residual_connections(input_ids)
    # self.debug_lm_head_output(input_ids)

    # è¿è¡Œè¿™äº›è°ƒè¯•æ¥ç¡®è®¤é—®é¢˜
    debug_mlp_implementation(self, input_ids)
    # self.check_mlp_weights(layer_idx=2)

    utils.print_rank_0("âœ… hf_model å…¨é¢è°ƒè¯•å®Œæˆ")


def debug_mlp_implementation(self, input_ids):
    """è¯¦ç»†è°ƒè¯•MLPå®ç°"""
    utils.print_rank_0("=== MLPå®ç°è¯¦ç»†è°ƒè¯• ===")

    hidden_states = self.model.embed_tokens(input_ids)
    attention_mask: Optional[torch.Tensor] = None
    # æ£€æŸ¥Rotary Embedding
    use_cache: Optional[bool] = None
    cache_position: Optional[torch.LongTensor] = None
    past_key_values: Optional[Cache] = None
    position_ids: Optional[torch.LongTensor] = None
    if use_cache and past_key_values is None:
        past_key_values = DynamicCache(config=self.model.config)

    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position = torch.arange(
            past_seen_tokens, past_seen_tokens + hidden_states.shape[1], device=hidden_states.device
        )

    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)

    rotary_pos_emb = self.model.rotary_emb(hidden_states, position_ids)
    cos, sin = rotary_pos_emb
    utils.print_rank_0(f"Rotary cos - å½¢çŠ¶: {cos.shape}, èŒƒå›´: [{cos.min():.3f}, {cos.max():.3f}]")
    utils.print_rank_0(f"Rotary sin - å½¢çŠ¶: {sin.shape}, èŒƒå›´: [{sin.min():.3f}, {sin.max():.3f}]")

    # It may already have been prepared by e.g. `generate`
    if not isinstance(causal_mask_mapping := attention_mask, dict):
        # Prepare mask arguments
        mask_kwargs = {
            "config": self.model.config,
            "input_embeds": hidden_states,
            "attention_mask": attention_mask,
            "cache_position": cache_position,
            "past_key_values": past_key_values,
            "position_ids": position_ids,
        }
        # Create the masks
        causal_mask_mapping = {
            "full_attention": create_causal_mask(**mask_kwargs),
        }

    # åªè¿è¡Œåˆ°ç¬¬2å±‚
    for layer_idx in range(3):
        layer = self.model.layers[layer_idx]
        hidden_states = layer.input_layernorm(hidden_states)

        # è¿è¡Œåˆ°MLPè¾“å…¥
        attention_output = layer.self_attn(hidden_states=hidden_states,
                                           position_embeddings=rotary_pos_emb,
                                           attention_mask=causal_mask_mapping[layer.attention_type],
                                           past_key_values=past_key_values,
                                           cache_position=cache_position,
                                           use_cache=use_cache,
                                           position_ids=position_ids,
                                           )
        if isinstance(attention_output, tuple):
            attention_output = attention_output[0]
        residual1 = hidden_states + attention_output
        mlp_input = layer.post_attention_layernorm(residual1)

        if layer_idx <= 2:  # é‡ç‚¹è°ƒè¯•ç¬¬2å±‚
            utils.print_rank_0(f"\n--- ç¬¬{layer_idx}å±‚MLPè¯¦ç»†è°ƒè¯• ---")
            utils.print_rank_0(f"MLPè¾“å…¥: mean={mlp_input.mean():.6f}, std={mlp_input.std():.6f}")

            mlp = layer.mlp
            # æ£€æŸ¥MLPå„ç»„ä»¶
            utils.print_rank_0(f"MLPç±»å‹: {type(mlp)}")

            # 2. åˆ†ä¸ºgateå’Œup
            gate = mlp.gate_proj(mlp_input)
            up = mlp.up_proj(mlp_input)
            utils.print_rank_0(f"gateéƒ¨åˆ†: shape={gate.shape}, mean={gate.mean():.6f}, std={gate.std():.6f}")
            utils.print_rank_0(f"upéƒ¨åˆ†: shape={up.shape}, mean={up.mean():.6f}, std={up.std():.6f}")

            # 3. æ¿€æ´»å‡½æ•°
            activated_gate = mlp.act_fn(gate)
            utils.print_rank_0(f"SiLU(gate): mean={activated_gate.mean():.6f}, std={activated_gate.std():.6f}")

            # 4. é—¨æ§ä¹˜æ³•
            intermediate = activated_gate * up
            utils.print_rank_0(f"gate * up: mean={intermediate.mean():.6f}, std={intermediate.std():.6f}")

            # 5. linear_fc2
            fc2_output = mlp.down_proj(intermediate)
            utils.print_rank_0(f"linear_fc2è¾“å‡º: mean={fc2_output.mean():.6f}, std={fc2_output.std():.6f}")

            # 6. dropout (å¦‚æœæœ‰)
            if hasattr(mlp, 'dropout') and mlp.dropout.p > 0:
                final_output = mlp.dropout(fc2_output)
                utils.print_rank_0(f"Dropoutå: mean={final_output.mean():.6f}, std={final_output.std():.6f}")
            else:
                final_output = fc2_output

            utils.print_rank_0(f"MLPæœ€ç»ˆè¾“å‡º: mean={final_output.mean():.6f}, std={final_output.std():.6f}")

        # å®Œæˆè¿™ä¸€å±‚
        layer_output = layer(hidden_states,
                             attention_mask=causal_mask_mapping[layer.attention_type],
                             position_ids=position_ids,
                             past_key_values=past_key_values,
                             use_cache=use_cache,
                             cache_position=cache_position,
                             position_embeddings=rotary_pos_emb,
                             )
        if isinstance(layer_output, tuple):
            hidden_states = layer_output[0]
        else:
            hidden_states = layer_output


def debug_attention_mechanism(self, input_ids):
    """è°ƒè¯•æ³¨æ„åŠ›æœºåˆ¶"""
    utils.print_rank_0("=== æ³¨æ„åŠ›æœºåˆ¶è°ƒè¯• ===")

    hidden_states = self.model.embed_tokens(input_ids)
    attention_mask: Optional[torch.Tensor] = None
    # æ£€æŸ¥Rotary Embedding
    use_cache: Optional[bool] = None
    cache_position: Optional[torch.LongTensor] = None
    past_key_values: Optional[Cache] = None
    position_ids: Optional[torch.LongTensor] = None
    if use_cache and past_key_values is None:
        past_key_values = DynamicCache(config=self.model.config)

    if cache_position is None:
        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0
        cache_position = torch.arange(
            past_seen_tokens, past_seen_tokens + hidden_states.shape[1], device=hidden_states.device
        )

    if position_ids is None:
        position_ids = cache_position.unsqueeze(0)

    rotary_pos_emb = self.model.rotary_emb(hidden_states, position_ids)
    cos, sin = rotary_pos_emb
    utils.print_rank_0(f"Rotary cos - å½¢çŠ¶: {cos.shape}, èŒƒå›´: [{cos.min():.3f}, {cos.max():.3f}]")
    utils.print_rank_0(f"Rotary sin - å½¢çŠ¶: {sin.shape}, èŒƒå›´: [{sin.min():.3f}, {sin.max():.3f}]")

    # It may already have been prepared by e.g. `generate`
    if not isinstance(causal_mask_mapping := attention_mask, dict):
        # Prepare mask arguments
        mask_kwargs = {
            "config": self.model.config,
            "input_embeds": hidden_states,
            "attention_mask": attention_mask,
            "cache_position": cache_position,
            "past_key_values": past_key_values,
            "position_ids": position_ids,
        }
        # Create the masks
        causal_mask_mapping = {
            "full_attention": create_causal_mask(**mask_kwargs),
        }

    for layer_idx in range(3):  # åªæ£€æŸ¥å‰3å±‚
        utils.print_rank_0(f"\n--- ç¬¬{layer_idx}å±‚æ³¨æ„åŠ›è°ƒè¯• ---")
        layer = self.model.layers[layer_idx]

        # æ£€æŸ¥è¾“å…¥
        input_stats = f"è¾“å…¥: mean={hidden_states.mean():.6f}, std={hidden_states.std():.6f}"

        # æ³¨æ„åŠ›å±‚å‰å‘ä¼ æ’­
        attention_output = layer.self_attn(hidden_states=hidden_states,
                                           position_embeddings=rotary_pos_emb,
                                           attention_mask=causal_mask_mapping[layer.attention_type],
                                           past_key_values=past_key_values,
                                           cache_position=cache_position,
                                           use_cache=use_cache,
                                           position_ids=position_ids,
                                           )

        if isinstance(attention_output, tuple):
            attention_output = attention_output[0]

        utils.print_rank_0(
            f"{input_stats} -> æ³¨æ„åŠ›è¾“å‡º: mean={attention_output.mean():.6f}, std={attention_output.std():.6f}")

        # æ£€æŸ¥æ³¨æ„åŠ›æƒé‡
        if hasattr(layer.self_attn, 'attention_scores'):
            scores = layer.self_attn.attention_scores
            utils.print_rank_0(f"æ³¨æ„åŠ›åˆ†æ•°èŒƒå›´: [{scores.min():.3f}, {scores.max():.3f}]")

        # ç»§ç»­å®Œæ•´å±‚çš„å‰å‘ä¼ æ’­
        layer_output = layer(hidden_states,
                             attention_mask=causal_mask_mapping[layer.attention_type],
                             position_ids=position_ids,
                             past_key_values=past_key_values,
                             use_cache=use_cache,
                             cache_position=cache_position,
                             position_embeddings=rotary_pos_emb,
                             )
        if isinstance(layer_output, tuple):
            hidden_states = layer_output[0]
        else:
            hidden_states = layer_output
