# 说明：仅保留支撑「大模型加载+分布式训练+数据处理+RL微调」的核心包，间接依赖会自动安装

# 1. 基础框架（必选：模型训练/计算核心）
torch==2.2.2+cu121            # PyTorch核心（GPU计算基础）
torchdata==0.11.0             # PyTorch数据加载工具（大模型数据管道必需）
transformers==4.57.1          # Hugging Face大模型加载（如LLaMA/OPT，核心依赖）
tokenizers==0.22.1            # 高效分词器（transformers配套，手动安装更稳定）
safetensors==0.6.2            # 安全模型权重加载（替代pickle，大模型常用）
tiktoken==0.12.0              # OpenAI风格token计数（PPO训练中样本截断必需）

# 2. 分布式训练（必选：多卡/大模型并行）
deepspeed==0.18.0             # 分布式训练框架（ZeRO优化、混合精度）
megatron-core @ file:///Users/huangdayi/project/github/Megatron-LM  # 本地Megatron核心（大模型并行关键）
Megatron==0.5.1               # Megatron配套工具（与megatron-core协同，确保并行配置）

# 3. 数据处理（必选：数据集加载/解析）
datasets==4.2.0               # Hugging Face数据集管理（RLHF数据加载）
pyarrow==14.0.1               # Parquet文件解析（数据格式，核心依赖）

# 4. 配置管理（必选：训练参数配置）
omegaconf==2.3.0              # 配置文件解析（处理YAML/JSON配置）
hydra-core==1.3.2             # 配置组合/动态参数（与OmegaConf配合，大模型训练常用）

# 5. RL微调（必选：PPO核心功能）
peft==0.11.1                  # 参数高效微调（PPO训练中模型更新，避免全量微调）

# 6. 兼容与工具（必选+可选）
typing_extensions==4.15.0     # Python3.10类型兼容（解决LiteralString/Sentinel缺失）
tqdm==4.67.1                  # 训练进度条（可选，提升体验，手动安装更灵活）
wandb==0.22.3                 # 实验跟踪（可选：日志/指标可视化，按需保留）

# 7. retriever 服务依赖
google-api-core==2.28.1
google-api-python-client==2.187.0
google-auth==2.43.0
google-auth-httplib2==0.2.1
googleapis-common-protos==1.72.0
uvicorn==0.38.0
fastapi==0.121.2

# PyTorch官方源（确保安装GPU版本，避免CPU版兼容问题）
--extra-index-url https://download.pytorch.org/whl/cu121